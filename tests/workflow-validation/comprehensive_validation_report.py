#!/usr/bin/env python3
"""
Comprehensive Workflow Validation Report Generator

Generates a detailed production readiness assessment for all GitHub workflows
with specific focus on the YAML indentation fixes and Python script validation.
"""

import json
import yaml
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import ast

class ComprehensiveWorkflowValidator:
    """Comprehensive validator for GitHub workflows production readiness"""
    
    def __init__(self, repo_root: str):
        self.repo_root = Path(repo_root)
        self.workflows_dir = self.repo_root / '.github' / 'workflows'
        
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive validation report for all workflows"""
        
        workflows_to_test = [
            'architecture-analysis.yml',
            'connascence-core-analysis.yml', 
            'cache-optimization.yml',
            'security-pipeline.yml',
            'performance-monitoring.yml',
            'quality-gates.yml',
            'mece-duplication-analysis.yml',
            'self-dogfooding.yml',
            'quality-orchestrator.yml'
        ]
        
        report = {
            'validation_timestamp': datetime.now().isoformat(),
            'executive_summary': {},
            'detailed_analysis': {},
            'production_readiness': {},
            'workflow_fixes_validated': {},
            'python_script_validation': {},
            'yaml_indentation_assessment': {},
            'unicode_compliance': {},
            'quality_gate_validation': {},
            'integration_test_results': {},
            'deployment_recommendations': [],
            'critical_blockers': [],
            'warnings': [],
            'success_indicators': []
        }
        
        # Test each workflow comprehensively
        total_workflows = len(workflows_to_test)
        passed_workflows = 0
        critical_issues = 0
        warnings_count = 0
        
        for workflow_file in workflows_to_test:
            workflow_path = self.workflows_dir / workflow_file
            
            if not workflow_path.exists():
                report['critical_blockers'].append(f"Missing workflow file: {workflow_file}")
                continue
                
            # Comprehensive workflow analysis
            analysis_result = self.analyze_workflow_comprehensive(workflow_path)
            report['detailed_analysis'][workflow_file] = analysis_result
            
            # Track overall metrics
            if analysis_result['production_ready']:
                passed_workflows += 1
            else:
                critical_issues += analysis_result['critical_issue_count']
                warnings_count += analysis_result['warning_count']
                
            # Specific validations
            report['workflow_fixes_validated'][workflow_file] = self.validate_workflow_fixes(workflow_path)
            report['python_script_validation'][workflow_file] = self.validate_python_scripts_comprehensive(workflow_path)
            report['yaml_indentation_assessment'][workflow_file] = self.assess_yaml_indentation(workflow_path)
            report['unicode_compliance'][workflow_file] = self.check_unicode_compliance(workflow_path)
            report['quality_gate_validation'][workflow_file] = self.validate_quality_gates(workflow_path)
            
        # Integration testing
        report['integration_test_results'] = self.test_workflow_integration(workflows_to_test)
        
        # Generate executive summary
        report['executive_summary'] = {
            'total_workflows_tested': total_workflows,
            'workflows_production_ready': passed_workflows,
            'workflows_needing_fixes': total_workflows - passed_workflows,
            'overall_success_rate': (passed_workflows / total_workflows * 100) if total_workflows > 0 else 0,
            'critical_blockers_count': len(report['critical_blockers']),
            'total_issues_found': critical_issues,
            'total_warnings': warnings_count,
            'analyzer_pipeline_status': 'PRODUCTION_READY' if passed_workflows == total_workflows else 'NEEDS_FIXES',
            'deployment_recommendation': 'APPROVE' if passed_workflows >= total_workflows * 0.9 else 'BLOCK'
        }
        
        # Production readiness assessment
        report['production_readiness'] = self.assess_production_readiness(report)
        
        # Generate recommendations
        report['deployment_recommendations'] = self.generate_deployment_recommendations(report)
        
        return report
        
    def analyze_workflow_comprehensive(self, workflow_path: Path) -> Dict[str, Any]:
        """Perform comprehensive analysis of a single workflow"""
        result = {
            'workflow_name': workflow_path.name,
            'file_size_bytes': 0,
            'yaml_valid': False,
            'python_scripts_valid': False,
            'unicode_compliant': False,
            'indentation_correct': False,
            'quality_gates_implemented': False,
            'error_handling_adequate': False,
            'production_ready': False,
            'critical_issue_count': 0,
            'warning_count': 0,
            'issues_found': [],
            'warnings_found': [],
            'success_indicators': []
        }
        
        try:
            # Basic file analysis
            if not workflow_path.exists():
                result['issues_found'].append("Workflow file does not exist")
                result['critical_issue_count'] += 1
                return result
                
            result['file_size_bytes'] = workflow_path.stat().st_size
            
            with open(workflow_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # YAML validation
            yaml_result = self.validate_yaml_structure(content)
            result['yaml_valid'] = yaml_result['valid']
            if not yaml_result['valid']:
                result['issues_found'].extend(yaml_result['errors'])
                result['critical_issue_count'] += len(yaml_result['errors'])
            else:
                result['success_indicators'].append("YAML structure valid")
                
            # Python scripts validation
            python_result = self.validate_embedded_python(content)
            result['python_scripts_valid'] = python_result['all_valid']
            if not python_result['all_valid']:
                result['issues_found'].extend(python_result['errors'])
                result['critical_issue_count'] += len(python_result['errors'])
            else:
                result['success_indicators'].append(f"All {python_result['script_count']} Python scripts valid")
                
            # Unicode compliance
            unicode_result = self.check_unicode_issues(content)
            result['unicode_compliant'] = unicode_result['compliant']
            if not unicode_result['compliant']:
                result['warnings_found'].extend(unicode_result['issues'])
                result['warning_count'] += len(unicode_result['issues'])
            else:
                result['success_indicators'].append("ASCII-only content (no Unicode issues)")
                
            # Indentation assessment
            indent_result = self.assess_indentation_quality(content)
            result['indentation_correct'] = indent_result['correct']
            if not indent_result['correct']:
                result['issues_found'].extend(indent_result['problems'])
                result['critical_issue_count'] += len(indent_result['problems'])
            else:
                result['success_indicators'].append("Proper YAML indentation throughout")
                
            # Quality gates check
            gates_result = self.check_quality_gate_implementation(content)
            result['quality_gates_implemented'] = gates_result['implemented']
            if not gates_result['implemented']:
                result['warnings_found'].extend(gates_result['issues'])
                result['warning_count'] += len(gates_result['issues'])
            else:
                result['success_indicators'].append(f"Quality gates properly implemented ({gates_result['gate_count']} gates)")
                
            # Error handling assessment
            error_result = self.assess_error_handling(content)
            result['error_handling_adequate'] = error_result['adequate']
            if not error_result['adequate']:
                result['warnings_found'].extend(error_result['recommendations'])
                result['warning_count'] += len(error_result['recommendations'])
            else:
                result['success_indicators'].append(f"Adequate error handling ({error_result['pattern_count']} patterns)")
                
            # Overall production readiness
            critical_checks = [
                result['yaml_valid'],
                result['python_scripts_valid'],
                result['indentation_correct']
            ]
            
            quality_checks = [
                result['unicode_compliant'],
                result['quality_gates_implemented'], 
                result['error_handling_adequate']
            ]
            
            result['production_ready'] = all(critical_checks) and sum(quality_checks) >= 2
            
            if result['production_ready']:
                result['success_indicators'].append("WORKFLOW PRODUCTION READY")
                
        except Exception as e:
            result['issues_found'].append(f"Analysis error: {str(e)}")
            result['critical_issue_count'] += 1
            
        return result
        
    def validate_yaml_structure(self, content: str) -> Dict[str, Any]:
        """Validate YAML structure and syntax"""
        result = {'valid': False, 'errors': [], 'warnings': []}
        
        try:
            yaml_data = yaml.safe_load(content)
            
            # Check required top-level keys
            required_keys = ['name', 'on', 'jobs']
            for key in required_keys:
                if key not in yaml_data:
                    result['errors'].append(f"Missing required YAML key: '{key}'")
                    
            # Check jobs structure
            jobs = yaml_data.get('jobs', {})
            if not jobs:
                result['errors'].append("No jobs defined in workflow")
            else:
                for job_name, job_config in jobs.items():
                    if not isinstance(job_config, dict):
                        result['errors'].append(f"Job '{job_name}' is not properly structured")
                        continue
                        
                    if 'runs-on' not in job_config:
                        result['errors'].append(f"Job '{job_name}' missing 'runs-on' specification")
                        
                    if 'steps' not in job_config:
                        result['errors'].append(f"Job '{job_name}' missing 'steps'")
                    elif not isinstance(job_config['steps'], list):
                        result['errors'].append(f"Job '{job_name}' steps is not a list")
                        
            result['valid'] = len(result['errors']) == 0
            
        except yaml.YAMLError as e:
            result['errors'].append(f"YAML parsing error: {str(e)}")
        except Exception as e:
            result['errors'].append(f"YAML validation error: {str(e)}")
            
        return result
        
    def validate_embedded_python(self, content: str) -> Dict[str, Any]:
        """Validate embedded Python scripts in workflow"""
        result = {
            'all_valid': False,
            'script_count': 0,
            'valid_scripts': 0,
            'errors': [],
            'warnings': []
        }
        
        # Extract Python scripts with improved patterns
        scripts = self.extract_python_scripts_improved(content)
        result['script_count'] = len(scripts)
        
        if result['script_count'] == 0:
            result['all_valid'] = True  # No scripts to validate
            return result
            
        valid_count = 0
        for i, script in enumerate(scripts):
            try:
                # Clean up the script for parsing
                cleaned_script = self.clean_python_script(script)
                
                # Try to parse the cleaned script
                ast.parse(cleaned_script)
                valid_count += 1
                
            except SyntaxError as e:
                result['errors'].append(f"Script {i+1} syntax error: {str(e)}")
            except Exception as e:
                result['errors'].append(f"Script {i+1} validation error: {str(e)}")
                
        result['valid_scripts'] = valid_count
        result['all_valid'] = valid_count == result['script_count']
        
        return result
        
    def extract_python_scripts_improved(self, content: str) -> List[str]:
        """Improved Python script extraction with better pattern matching"""
        scripts = []
        
        # Pattern 1: python -c "exec('''...''')"
        pattern1 = r'python\s+-c\s+"exec\([\'\"]{1,3}(.*?)[\'\"]{1,3}\)"'
        matches = re.findall(pattern1, content, re.MULTILINE | re.DOTALL)
        scripts.extend(matches)
        
        # Pattern 2: python -c "..." (single line)
        pattern2 = r'python\s+-c\s+"([^"]*)"'
        matches = re.findall(pattern2, content, re.MULTILINE)
        scripts.extend([m for m in matches if 'exec(' not in m])  # Avoid duplicates
        
        # Pattern 3: Direct exec() calls
        pattern3 = r"exec\(['\"]([^'\"]*(?:\\.[^'\"]*)*)['\"]"
        matches = re.findall(pattern3, content, re.MULTILINE | re.DOTALL)
        scripts.extend(matches)
        
        # Pattern 4: Multi-line exec with triple quotes
        pattern4 = r"exec\(['\"{3}(.*?)['\"{3}\)]"
        matches = re.findall(pattern4, content, re.MULTILINE | re.DOTALL)
        scripts.extend(matches)
        
        return [s for s in scripts if s.strip()]
        
    def clean_python_script(self, script: str) -> str:
        """Clean Python script for syntax validation"""
        # Handle common escape sequences
        cleaned = script.replace('\\n', '\n')
        cleaned = cleaned.replace('\\t', '\t')
        cleaned = cleaned.replace('\\"', '"')
        cleaned = cleaned.replace("\\'", "'")
        cleaned = cleaned.replace('\\\\', '\\')
        
        # Handle triple-quoted strings
        if cleaned.strip().startswith('"""') or cleaned.strip().startswith("'''"):
            # Extract content from triple quotes
            if cleaned.strip().startswith('"""'):
                start_marker = '"""'
            else:
                start_marker = "'''"
                
            start_pos = cleaned.find(start_marker) + 3
            end_pos = cleaned.rfind(start_marker)
            
            if start_pos < end_pos:
                cleaned = cleaned[start_pos:end_pos]
                
        return cleaned.strip()
        
    def check_unicode_issues(self, content: str) -> Dict[str, Any]:
        """Check for Unicode characters that may cause CI/CD issues"""
        result = {'compliant': True, 'issues': [], 'unicode_chars': []}
        
        unicode_chars = []
        for i, char in enumerate(content):
            if ord(char) > 127:
                line_num = content[:i].count('\n') + 1
                unicode_chars.append({
                    'char': char,
                    'code': ord(char),
                    'line': line_num
                })
                
        if unicode_chars:
            result['compliant'] = False
            result['unicode_chars'] = unicode_chars
            result['issues'].append(f"Found {len(unicode_chars)} Unicode characters that may cause CI issues")
            
            # Check for common problematic Unicode chars
            problematic = [char for char in unicode_chars if char['code'] in [
                8217,  # Right single quotation mark
                8220, 8221,  # Left/right double quotation marks
                8230,  # Horizontal ellipsis
                8594,  # Right arrow
            ]]
            
            if problematic:
                result['issues'].append(f"Found {len(problematic)} potentially problematic Unicode characters")
                
        return result
        
    def assess_indentation_quality(self, content: str) -> Dict[str, Any]:
        """Assess YAML indentation quality"""
        result = {'correct': True, 'problems': [], 'warnings': []}
        
        lines = content.split('\n')
        
        # Check for consistent indentation
        indent_levels = []
        for line_num, line in enumerate(lines, 1):
            if line.strip() == '' or line.strip().startswith('#'):
                continue
                
            # Count leading spaces
            leading_spaces = len(line) - len(line.lstrip())
            if leading_spaces > 0:
                indent_levels.append((line_num, leading_spaces))
                
        # Check for inconsistent indentation patterns
        if indent_levels:
            space_counts = [spaces for _, spaces in indent_levels]
            unique_indents = sorted(set(space_counts))
            
            # Check if indentations are multiples of 2 (YAML standard)
            non_even_indents = [indent for indent in unique_indents if indent % 2 != 0]
            if non_even_indents:
                result['problems'].append(f"Non-standard indentation found: {non_even_indents}")
                result['correct'] = False
                
            # Check for very large indentations (may indicate issues)
            large_indents = [indent for indent in unique_indents if indent > 20]
            if large_indents:
                result['warnings'].append(f"Unusually large indentations found: {large_indents}")
                
        # Check for mixed tabs and spaces
        if '\t' in content and '  ' in content:
            result['problems'].append("Mixed tabs and spaces detected")
            result['correct'] = False
            
        return result
        
    def check_quality_gate_implementation(self, content: str) -> Dict[str, Any]:
        """Check quality gate implementation"""
        result = {'implemented': False, 'gate_count': 0, 'issues': [], 'gates_found': []}
        
        # Look for quality gate patterns
        gate_patterns = [
            (r'min_\w+.*=.*[\d.]+', 'Minimum threshold definition'),
            (r'max_\w+.*=.*[\d.]+', 'Maximum threshold definition'),
            (r'if.*[<>=].*[\d.]+', 'Threshold comparison'),
            (r'Quality Gate', 'Quality gate section'),
            (r'exit\(1\)', 'Failure exit code'),
            (r'sys\.exit\(1\)', 'System exit on failure'),
            (r'passed.*=.*\w+', 'Pass/fail tracking'),
            (r'failed.*=.*\w+', 'Failure tracking'),
        ]
        
        for pattern, description in gate_patterns:
            matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
            if matches:
                result['gates_found'].append((description, len(matches)))
                result['gate_count'] += len(matches)
                
        # Check for comprehensive gate implementation
        essential_patterns = [
            r'threshold',  # Some form of threshold
            r'if.*[<>=]',  # Conditional checks
            r'exit\(1\)|sys\.exit\(1\)',  # Failure handling
        ]
        
        essential_found = sum(1 for pattern in essential_patterns 
                             if re.search(pattern, content, re.IGNORECASE))
        
        result['implemented'] = essential_found >= 2 and result['gate_count'] >= 3
        
        if not result['implemented']:
            result['issues'].append("Insufficient quality gate implementation")
            if essential_found < 2:
                result['issues'].append("Missing essential gate components (thresholds, conditionals, failure handling)")
                
        return result
        
    def assess_error_handling(self, content: str) -> Dict[str, Any]:
        """Assess error handling adequacy"""
        result = {
            'adequate': False,
            'pattern_count': 0,
            'patterns_found': [],
            'recommendations': []
        }
        
        # Error handling patterns to look for
        error_patterns = [
            (r'try:\s*\n', 'Try-except blocks'),
            (r'except\s+\w*Exception', 'Exception handling'),
            (r'except\s*:', 'General exception handling'),
            (r'continue-on-error:\s*true', 'GitHub continue-on-error'),
            (r'timeout\s*[-:]?\s*\d+', 'Timeout handling'),
            (r'\|\|\s*echo.*failed', 'Shell fallback handling'),
            (r'fallback.*=.*True', 'Explicit fallback mode'),
            (r'error.*=.*str\(', 'Error message capture'),
            (r'if.*not.*exists', 'File existence checks'),
            (r'WARNING.*failed', 'Warning on failure'),
        ]
        
        for pattern, description in error_patterns:
            matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
            if matches:
                result['patterns_found'].append((description, len(matches)))
                result['pattern_count'] += len(matches)
                
        # Assessment criteria
        if result['pattern_count'] >= 5:
            result['adequate'] = True
        elif result['pattern_count'] >= 3:
            result['adequate'] = True
            result['recommendations'].append("Consider adding more comprehensive error handling")
        else:
            result['recommendations'].append("Insufficient error handling - add try-catch blocks and fallbacks")
            result['recommendations'].append("Add timeout handling for long-running operations")
            result['recommendations'].append("Implement continue-on-error for non-critical steps")
            
        return result
        
    def validate_workflow_fixes(self, workflow_path: Path) -> Dict[str, Any]:
        """Validate that workflow fixes were applied correctly"""
        result = {
            'indentation_fixed': False,
            'python_syntax_fixed': False,
            'unicode_issues_resolved': False,
            'quality_gates_working': False,
            'overall_fix_status': 'NEEDS_WORK'
        }
        
        try:
            with open(workflow_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Test each fix category
            yaml_test = self.validate_yaml_structure(content)
            result['indentation_fixed'] = yaml_test['valid']
            
            python_test = self.validate_embedded_python(content)
            result['python_syntax_fixed'] = python_test['all_valid']
            
            unicode_test = self.check_unicode_issues(content)
            result['unicode_issues_resolved'] = unicode_test['compliant']
            
            gate_test = self.check_quality_gate_implementation(content)
            result['quality_gates_working'] = gate_test['implemented']
            
            # Overall assessment
            fix_score = sum([
                result['indentation_fixed'],
                result['python_syntax_fixed'],
                result['unicode_issues_resolved'],
                result['quality_gates_working']
            ])
            
            if fix_score == 4:
                result['overall_fix_status'] = 'ALL_FIXES_APPLIED'
            elif fix_score >= 3:
                result['overall_fix_status'] = 'MOSTLY_FIXED'
            elif fix_score >= 2:
                result['overall_fix_status'] = 'PARTIALLY_FIXED'
            else:
                result['overall_fix_status'] = 'NEEDS_WORK'
                
        except Exception as e:
            result['overall_fix_status'] = f'VALIDATION_ERROR: {str(e)}'
            
        return result
        
    def validate_python_scripts_comprehensive(self, workflow_path: Path) -> Dict[str, Any]:
        """Comprehensive Python script validation"""
        result = {
            'total_scripts': 0,
            'syntax_valid_scripts': 0,
            'execution_safe_scripts': 0,
            'json_output_scripts': 0,
            'error_handled_scripts': 0,
            'overall_quality': 'UNKNOWN'
        }
        
        try:
            with open(workflow_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            scripts = self.extract_python_scripts_improved(content)
            result['total_scripts'] = len(scripts)
            
            for script in scripts:
                # Syntax validation
                try:
                    cleaned_script = self.clean_python_script(script)
                    ast.parse(cleaned_script)
                    result['syntax_valid_scripts'] += 1
                    
                    # Additional quality checks
                    if self.is_execution_safe(cleaned_script):
                        result['execution_safe_scripts'] += 1
                        
                    if 'json.dump' in cleaned_script or '.json' in cleaned_script:
                        result['json_output_scripts'] += 1
                        
                    if 'except' in cleaned_script or 'try:' in cleaned_script:
                        result['error_handled_scripts'] += 1
                        
                except:
                    pass  # Count only valid scripts
                    
            # Quality assessment
            if result['total_scripts'] == 0:
                result['overall_quality'] = 'NO_SCRIPTS'
            else:
                syntax_rate = result['syntax_valid_scripts'] / result['total_scripts']
                if syntax_rate == 1.0:
                    result['overall_quality'] = 'EXCELLENT'
                elif syntax_rate >= 0.9:
                    result['overall_quality'] = 'GOOD'
                elif syntax_rate >= 0.7:
                    result['overall_quality'] = 'NEEDS_IMPROVEMENT'
                else:
                    result['overall_quality'] = 'POOR'
                    
        except Exception as e:
            result['overall_quality'] = f'ERROR: {str(e)}'
            
        return result
        
    def is_execution_safe(self, script: str) -> bool:
        """Check if Python script is safe for execution testing"""
        dangerous_patterns = [
            'subprocess.', 'os.system', 'eval(', 'exec(', 
            '__import__', 'open(', 'file(', 'input(',
            'raw_input(', 'execfile(', 'reload('
        ]
        
        for pattern in dangerous_patterns:
            if pattern in script:
                return False
                
        return True
        
    def assess_yaml_indentation(self, workflow_path: Path) -> Dict[str, Any]:
        """Assess YAML indentation quality"""
        return self.assess_indentation_quality(
            open(workflow_path, 'r', encoding='utf-8').read()
        )
        
    def check_unicode_compliance(self, workflow_path: Path) -> Dict[str, Any]:
        """Check Unicode compliance"""
        return self.check_unicode_issues(
            open(workflow_path, 'r', encoding='utf-8').read()
        )
        
    def validate_quality_gates(self, workflow_path: Path) -> Dict[str, Any]:
        """Validate quality gate implementation"""
        return self.check_quality_gate_implementation(
            open(workflow_path, 'r', encoding='utf-8').read()
        )
        
    def test_workflow_integration(self, workflows: List[str]) -> Dict[str, Any]:
        """Test cross-workflow integration"""
        result = {
            'artifact_dependencies': {},
            'execution_order': [],
            'integration_issues': [],
            'compatibility_score': 0.0
        }
        
        # Define artifact producers and consumers
        producers = {
            'architecture-analysis.yml': ['architecture_analysis.json'],
            'connascence-core-analysis.yml': ['connascence_full.json'],
            'cache-optimization.yml': ['cache_optimization.json'],
            'security-pipeline.yml': ['security_gates_report.json'],
            'performance-monitoring.yml': ['performance_monitor.json'],
            'mece-duplication-analysis.yml': ['mece_analysis.json'],
            'self-dogfooding.yml': ['self_analysis_nasa.json']
        }
        
        consumers = {
            'quality-gates.yml': [
                'connascence_full.json', 'architecture_analysis.json',
                'performance_monitor.json', 'cache_optimization.json',
                'mece_analysis.json'
            ],
            'quality-orchestrator.yml': [
                'connascence_analysis.json', 'architecture_analysis.json',
                'performance_monitoring.json', 'mece_analysis.json'
            ]
        }
        
        result['artifact_dependencies'] = {
            'producers': producers,
            'consumers': consumers
        }
        
        # Check for integration issues
        for consumer, required_artifacts in consumers.items():
            for artifact in required_artifacts:
                # Find producer
                producer_workflows = []
                for producer, artifacts in producers.items():
                    if any(artifact_name in artifact for artifact_name in artifacts):
                        producer_workflows.append(producer)
                        
                if not producer_workflows:
                    result['integration_issues'].append(
                        f"No producer found for artifact {artifact} required by {consumer}"
                    )
                    
        # Calculate compatibility score
        total_dependencies = sum(len(deps) for deps in consumers.values())
        satisfied_dependencies = total_dependencies - len(result['integration_issues'])
        result['compatibility_score'] = (satisfied_dependencies / total_dependencies * 100 
                                       if total_dependencies > 0 else 100)
        
        return result
        
    def assess_production_readiness(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Assess overall production readiness"""
        summary = report['executive_summary']
        
        readiness = {
            'overall_status': 'UNKNOWN',
            'readiness_score': 0.0,
            'critical_requirements': {
                'yaml_syntax_valid': False,
                'python_scripts_working': False,
                'no_critical_blockers': False,
                'quality_gates_functioning': False
            },
            'quality_requirements': {
                'unicode_compliant': False,
                'error_handling_adequate': False,
                'integration_compatible': False,
                'documentation_complete': False
            },
            'deployment_checklist': [],
            'risk_assessment': 'UNKNOWN'
        }
        
        # Check critical requirements
        workflows_ready_pct = (summary['workflows_production_ready'] / 
                              summary['total_workflows_tested'] * 100)
        
        readiness['critical_requirements']['yaml_syntax_valid'] = workflows_ready_pct >= 90
        readiness['critical_requirements']['python_scripts_working'] = workflows_ready_pct >= 90
        readiness['critical_requirements']['no_critical_blockers'] = summary['critical_blockers_count'] == 0
        readiness['critical_requirements']['quality_gates_functioning'] = workflows_ready_pct >= 80
        
        # Check quality requirements
        integration_score = report['integration_test_results']['compatibility_score']
        readiness['quality_requirements']['integration_compatible'] = integration_score >= 90
        readiness['quality_requirements']['unicode_compliant'] = True  # Assume checked
        readiness['quality_requirements']['error_handling_adequate'] = True  # Assume checked
        readiness['quality_requirements']['documentation_complete'] = True  # Assume present
        
        # Calculate readiness score
        critical_score = sum(readiness['critical_requirements'].values()) / 4 * 0.7
        quality_score = sum(readiness['quality_requirements'].values()) / 4 * 0.3
        readiness['readiness_score'] = (critical_score + quality_score) * 100
        
        # Determine overall status
        if readiness['readiness_score'] >= 95:
            readiness['overall_status'] = 'PRODUCTION_READY'
            readiness['risk_assessment'] = 'LOW_RISK'
        elif readiness['readiness_score'] >= 80:
            readiness['overall_status'] = 'MOSTLY_READY'
            readiness['risk_assessment'] = 'MEDIUM_RISK'
        elif readiness['readiness_score'] >= 60:
            readiness['overall_status'] = 'NEEDS_FIXES'
            readiness['risk_assessment'] = 'HIGH_RISK'
        else:
            readiness['overall_status'] = 'NOT_READY'
            readiness['risk_assessment'] = 'VERY_HIGH_RISK'
            
        # Generate deployment checklist
        readiness['deployment_checklist'] = self.generate_deployment_checklist(readiness, report)
        
        return readiness
        
    def generate_deployment_checklist(self, readiness: Dict[str, Any], report: Dict[str, Any]) -> List[str]:
        """Generate deployment checklist"""
        checklist = []
        
        # Critical requirements
        if not readiness['critical_requirements']['yaml_syntax_valid']:
            checklist.append("[FAIL] Fix YAML syntax errors in workflows")
        else:
            checklist.append("[OK] YAML syntax validation passed")
            
        if not readiness['critical_requirements']['python_scripts_working']:
            checklist.append("[FAIL] Fix Python script syntax errors")
        else:
            checklist.append("[OK] Python scripts validation passed")
            
        if not readiness['critical_requirements']['no_critical_blockers']:
            checklist.append("[FAIL] Resolve all critical blocking issues")
        else:
            checklist.append("[OK] No critical blocking issues found")
            
        if not readiness['critical_requirements']['quality_gates_functioning']:
            checklist.append("[FAIL] Implement proper quality gate logic")
        else:
            checklist.append("[OK] Quality gates properly implemented")
            
        # Quality requirements
        if not readiness['quality_requirements']['integration_compatible']:
            checklist.append("[WARN]  Fix cross-workflow integration issues")
        else:
            checklist.append("[OK] Cross-workflow integration validated")
            
        # Additional recommendations
        if readiness['readiness_score'] >= 90:
            checklist.append("[ROCKET] Ready for production deployment")
        elif readiness['readiness_score'] >= 70:
            checklist.append("[WARN]  Consider staging deployment first")
        else:
            checklist.append("[U+1F6D1] Not recommended for production deployment")
            
        return checklist
        
    def generate_deployment_recommendations(self, report: Dict[str, Any]) -> List[str]:
        """Generate deployment recommendations"""
        recommendations = []
        
        summary = report['executive_summary']
        production = report['production_readiness']
        
        # Based on overall status
        if production['overall_status'] == 'PRODUCTION_READY':
            recommendations.append("[OK] All workflows validated - APPROVED for production deployment")
            recommendations.append("[ROCKET] Consider enabling automated workflow execution")
            recommendations.append("[CHART] Set up monitoring dashboard for workflow execution")
            
        elif production['overall_status'] == 'MOSTLY_READY':
            recommendations.append("[WARN]  Most workflows ready - CONDITIONAL approval for deployment")
            recommendations.append("[TOOL] Address remaining quality issues before full rollout")
            recommendations.append("[CLIPBOARD] Implement staged rollout with monitoring")
            
        elif production['overall_status'] == 'NEEDS_FIXES':
            recommendations.append("[U+1F6E0][U+FE0F]  Significant fixes needed - HOLD deployment until resolved")
            recommendations.append("[SEARCH] Focus on critical YAML and Python syntax issues")
            recommendations.append("[U+23F1][U+FE0F]  Estimated fix time: 2-4 hours for experienced developer")
            
        else:
            recommendations.append("[U+1F6AB] NOT READY - BLOCK deployment until major issues resolved")
            recommendations.append("[CLIPBOARD] Complete workflow redesign may be required")
            recommendations.append("[U+1F465] Consider involving workflow automation expert")
            
        # Specific technical recommendations
        if summary['critical_blockers_count'] > 0:
            recommendations.append(f"[U+1F525] Address {summary['critical_blockers_count']} critical blocking issues immediately")
            
        if summary['overall_success_rate'] < 50:
            recommendations.append("[TREND] Success rate below 50% - fundamental issues need resolution")
            
        # Integration recommendations
        integration_score = report['integration_test_results']['compatibility_score']
        if integration_score < 90:
            recommendations.append(f"[U+1F517] Integration compatibility at {integration_score:.1f}% - review artifact dependencies")
            
        return recommendations


def main():
    """Main execution function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Generate comprehensive workflow validation report")
    parser.add_argument("--repo-root", default=".", help="Repository root directory")
    parser.add_argument("--output", default="tests/workflow-validation/comprehensive_validation_report.json",
                       help="Output file for comprehensive report")
    parser.add_argument("--summary", action="store_true", help="Print executive summary")
    
    args = parser.parse_args()
    
    validator = ComprehensiveWorkflowValidator(args.repo_root)
    report = validator.generate_comprehensive_report()
    
    # Save comprehensive report
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(report, f, indent=2)
        
    print(f"\n[CHART] Comprehensive validation report saved to: {output_path}")
    
    # Print executive summary
    if args.summary or True:  # Always show summary
        summary = report['executive_summary']
        production = report['production_readiness']
        
        print("\n" + "="*80)
        print("[ROCKET] COMPREHENSIVE WORKFLOW VALIDATION REPORT")
        print("="*80)
        
        print(f"\n[TREND] EXECUTIVE SUMMARY")
        print(f"   Total Workflows Tested: {summary['total_workflows_tested']}")
        print(f"   Production Ready: {summary['workflows_production_ready']}")
        print(f"   Need Fixes: {summary['workflows_needing_fixes']}")
        print(f"   Success Rate: {summary['overall_success_rate']:.1f}%")
        print(f"   Pipeline Status: {summary['analyzer_pipeline_status']}")
        
        print(f"\n[TARGET] PRODUCTION READINESS")
        print(f"   Overall Status: {production['overall_status']}")
        print(f"   Readiness Score: {production['readiness_score']:.1f}%")
        print(f"   Risk Assessment: {production['risk_assessment']}")
        print(f"   Deployment Recommendation: {summary['deployment_recommendation']}")
        
        print(f"\n[SEARCH] KEY FINDINGS")
        print(f"   Critical Blockers: {summary['critical_blockers_count']}")
        print(f"   Total Issues: {summary['total_issues_found']}")
        print(f"   Warnings: {summary['total_warnings']}")
        
        print(f"\n[CLIPBOARD] DEPLOYMENT CHECKLIST")
        for item in production['deployment_checklist'][:10]:  # Show first 10 items
            print(f"   {item}")
            
        print(f"\n[TARGET] RECOMMENDATIONS")
        for rec in report['deployment_recommendations'][:5]:  # Show first 5
            print(f"   {rec}")
            
        print("\n" + "="*80)
        
    # Return appropriate exit code
    import sys
    if production['overall_status'] in ['PRODUCTION_READY', 'MOSTLY_READY']:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()