#!/usr/bin/env python3
"""
Python Script Execution Tests for GitHub Workflows

Tests all embedded Python scripts in workflows to ensure they execute correctly
without syntax errors, import issues, or runtime failures in simulated CI environment.
"""

import ast
import json
import os
import sys
import tempfile
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Any
import subprocess
import re

class PythonScriptTester:
    """Tests Python scripts embedded in GitHub workflows"""
    
    def __init__(self, repo_root: str):
        self.repo_root = Path(repo_root)
        self.workflows_dir = self.repo_root / '.github' / 'workflows'
        
    def test_all_python_scripts(self) -> Dict[str, Any]:
        """Test all Python scripts in all workflows"""
        workflows = [
            'architecture-analysis.yml',
            'connascence-core-analysis.yml',
            'cache-optimization.yml', 
            'security-pipeline.yml',
            'performance-monitoring.yml',
            'quality-gates.yml',
            'mece-duplication-analysis.yml',
            'self-dogfooding.yml',
            'quality-orchestrator.yml'
        ]
        
        results = {
            'summary': {
                'total_workflows': 0,
                'total_scripts': 0,
                'syntax_valid': 0,
                'execution_tested': 0,
                'execution_successful': 0
            },
            'workflow_results': {},
            'critical_issues': [],
            'recommendations': []
        }
        
        for workflow in workflows:
            workflow_path = self.workflows_dir / workflow
            if workflow_path.exists():
                result = self.test_workflow_scripts(workflow_path)
                results['workflow_results'][workflow] = result
                results['summary']['total_workflows'] += 1
                results['summary']['total_scripts'] += result['script_count']
                results['summary']['syntax_valid'] += result['syntax_valid']
                results['summary']['execution_tested'] += result['execution_tested']
                results['summary']['execution_successful'] += result['execution_successful']
                
                if result['critical_issues']:
                    results['critical_issues'].extend(f"{workflow}: {issue}" for issue in result['critical_issues'])
                    
        # Generate recommendations
        results['recommendations'] = self.generate_recommendations(results)
        
        return results
        
    def test_workflow_scripts(self, workflow_path: Path) -> Dict[str, Any]:
        """Test all Python scripts in a single workflow"""
        with open(workflow_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        scripts = self.extract_python_scripts(content)
        
        result = {
            'script_count': len(scripts),
            'syntax_valid': 0,
            'execution_tested': 0,
            'execution_successful': 0,
            'scripts': [],
            'critical_issues': [],
            'warnings': []
        }
        
        for i, script in enumerate(scripts):
            script_result = self.test_single_script(script, f"script_{i+1}")
            result['scripts'].append(script_result)
            
            if script_result['syntax_valid']:
                result['syntax_valid'] += 1
                
            if script_result['execution_tested']:
                result['execution_tested'] += 1
                
            if script_result['execution_successful']:
                result['execution_successful'] += 1
                
            if script_result['critical_issues']:
                result['critical_issues'].extend(script_result['critical_issues'])
                
            if script_result['warnings']:
                result['warnings'].extend(script_result['warnings'])
                
        return result
        
    def extract_python_scripts(self, content: str) -> List[str]:
        """Extract Python scripts from workflow content"""
        scripts = []
        
        # Pattern 1: python -c "exec('''...''')"
        pattern1 = r'python\s+-c\s+"exec\([\'\"]{3}(.*?)[\'\"]{3}\)"'
        matches1 = re.findall(pattern1, content, re.MULTILINE | re.DOTALL)
        scripts.extend(matches1)
        
        # Pattern 2: python -c "..."
        pattern2 = r'python\s+-c\s+"([^"]*(?:\\.[^"]*)*)"'
        matches2 = re.findall(pattern2, content, re.MULTILINE | re.DOTALL)
        scripts.extend(matches2)
        
        # Pattern 3: exec('''...''')
        pattern3 = r"exec\(['\"]([^'\"]*(?:\\.[^'\"]*)*)['\"]"
        matches3 = re.findall(pattern3, content, re.MULTILINE | re.DOTALL)
        scripts.extend(matches3)
        
        # Clean and filter scripts
        cleaned_scripts = []
        for script in scripts:
            # Skip if already processed (avoid duplicates)
            if script in cleaned_scripts:
                continue
                
            # Clean up escape sequences
            cleaned = script.replace('\\"', '"').replace("\\'", "'")
            cleaned = cleaned.replace('\\n', '\n').replace('\\t', '\t')
            cleaned = cleaned.replace('\\\\', '\\')
            
            # Remove exec wrapper if present
            if cleaned.strip().startswith('exec('):
                # Extract content from exec()
                try:
                    exec_content = cleaned.strip()
                    if exec_content.startswith('exec('):
                        start = exec_content.find('(') + 1
                        end = exec_content.rfind(')')
                        if start < end:
                            inner = exec_content[start:end]
                            # Remove surrounding quotes
                            if ((inner.startswith('"""') and inner.endswith('"""')) or 
                                (inner.startswith("'''") and inner.endswith("'''"))):
                                cleaned = inner[3:-3]
                            elif ((inner.startswith('"') and inner.endswith('"')) or 
                                  (inner.startswith("'") and inner.endswith("'"))):
                                cleaned = inner[1:-1]
                            else:
                                cleaned = inner
                except:
                    pass  # Keep original if parsing fails
                    
            if cleaned.strip():
                cleaned_scripts.append(cleaned.strip())
                
        return cleaned_scripts
        
    def test_single_script(self, script: str, script_name: str) -> Dict[str, Any]:
        """Test a single Python script"""
        result = {
            'script_name': script_name,
            'script_length': len(script),
            'syntax_valid': False,
            'execution_tested': False,
            'execution_successful': False,
            'syntax_errors': [],
            'runtime_errors': [],
            'warnings': [],
            'critical_issues': [],
            'imports_detected': [],
            'outputs_generated': []
        }
        
        # Test 1: Syntax validation
        try:
            ast.parse(script)
            result['syntax_valid'] = True
        except SyntaxError as e:
            result['syntax_errors'].append(f"Line {e.lineno}: {e.msg}")
            result['critical_issues'].append(f"Syntax error: {e.msg}")
        except Exception as e:
            result['syntax_errors'].append(f"Parse error: {e}")
            result['critical_issues'].append(f"Parse error: {e}")
            
        if not result['syntax_valid']:
            return result
            
        # Test 2: Import analysis
        result['imports_detected'] = self.analyze_imports(script)
        
        # Test 3: Execution simulation
        if self.should_test_execution(script):
            result['execution_tested'] = True
            execution_result = self.simulate_execution(script, script_name)
            
            result['execution_successful'] = execution_result['success']
            result['runtime_errors'] = execution_result['errors']
            result['warnings'].extend(execution_result['warnings'])
            result['outputs_generated'] = execution_result['outputs']
            
            if not execution_result['success']:
                result['critical_issues'].extend(execution_result['errors'])
                
        # Test 4: Output validation
        if 'json.dump' in script or '.json' in script:
            json_validation = self.validate_json_output_logic(script)
            if not json_validation['valid']:
                result['warnings'].extend(json_validation['issues'])
                
        # Test 5: CI/CD compatibility
        ci_issues = self.check_ci_compatibility(script)
        if ci_issues:
            result['warnings'].extend(ci_issues)
            
        return result
        
    def analyze_imports(self, script: str) -> List[str]:
        """Analyze imports in the script"""
        imports = []
        
        try:
            tree = ast.parse(script)
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ''
                    for alias in node.names:
                        imports.append(f"{module}.{alias.name}" if module else alias.name)
        except:
            pass
            
        return imports
        
    def should_test_execution(self, script: str) -> bool:
        """Determine if script should be execution tested"""
        # Don't test scripts that:
        # - Have risky operations
        # - Require external dependencies
        # - Are too complex
        
        risky_patterns = [
            'subprocess.run',
            'os.system',
            'exec(',
            'eval(',
            '__import__',
            'open(',  # File operations might be risky in test
            'requests.',  # Network operations
            'urllib.'   # Network operations
        ]
        
        for pattern in risky_patterns:
            if pattern in script:
                return False
                
        # Test if script is simple enough
        if len(script) > 5000:  # Very large scripts
            return False
            
        return True
        
    def simulate_execution(self, script: str, script_name: str) -> Dict[str, Any]:
        """Simulate script execution in safe environment"""
        result = {
            'success': False,
            'errors': [],
            'warnings': [],
            'outputs': []
        }
        
        # Create safe execution environment
        safe_globals = {
            '__builtins__': {
                'print': lambda *args, **kwargs: result['outputs'].append(' '.join(str(a) for a in args)),
                'len': len,
                'str': str,
                'int': int,
                'float': float,
                'bool': bool,
                'list': list,
                'dict': dict,
                'tuple': tuple,
                'set': set,
                'range': range,
                'enumerate': enumerate,
                'zip': zip,
                'max': max,
                'min': min,
                'sum': sum,
                'all': all,
                'any': any,
                'isinstance': isinstance,
                'hasattr': hasattr,
                'getattr': getattr,
                'setattr': setattr,
            }
        }
        
        # Mock common modules
        safe_globals.update({
            'json': MockJson(),
            'datetime': MockDateTime(),
            'os': MockOs(),
            'sys': MockSys(),
            'Path': MockPath,
            'pathlib': type('MockPathlib', (), {'Path': MockPath})()
        })
        
        try:
            # Execute in safe environment
            exec(script, safe_globals)
            result['success'] = True
        except ImportError as e:
            result['warnings'].append(f"Import not available in test environment: {e}")
            result['success'] = True  # Not critical for testing
        except Exception as e:
            result['errors'].append(f"Runtime error: {e}")
            
        return result
        
    def validate_json_output_logic(self, script: str) -> Dict[str, Any]:
        """Validate JSON output logic in script"""
        result = {'valid': True, 'issues': []}
        
        # Check for proper JSON structure
        json_patterns = [
            (r'json\.dump\([^,]+,\s*[^,]+\)', 'JSON dump with file'),
            (r'\.json"', 'JSON file extension'),
            (r'{[^}]*}', 'Dictionary structure'),
            (r'\[[^\]]*\]', 'List structure')
        ]
        
        json_operations = 0
        for pattern, description in json_patterns:
            if re.search(pattern, script):
                json_operations += 1
                
        if 'json' in script and json_operations < 2:
            result['issues'].append("JSON operations detected but structure unclear")
            
        # Check for proper error handling around JSON operations
        if 'json.dump' in script and 'try:' not in script:
            result['issues'].append("JSON dump without error handling")
            
        return result
        
    def check_ci_compatibility(self, script: str) -> List[str]:
        """Check CI/CD compatibility issues"""
        issues = []
        
        # Check for hardcoded paths
        if '/' in script and 'C:' not in script:  # Unix paths
            if '/home/' in script or '/usr/' in script:
                issues.append("Hardcoded system paths detected")
                
        # Check for Unicode issues
        try:
            script.encode('ascii')
        except UnicodeEncodeError:
            issues.append("Non-ASCII characters may cause CI issues")
            
        # Check for timezone dependencies
        if 'datetime.now()' in script and 'utc' not in script.lower():
            issues.append("Timezone-dependent datetime usage")
            
        # Check for large memory operations
        if 'range(' in script:
            range_match = re.search(r'range\(([^)]+)\)', script)
            if range_match:
                try:
                    range_expr = range_match.group(1)
                    if any(num in range_expr for num in ['1000000', '10**6', '1e6']):
                        issues.append("Large range operations may consume excessive memory")
                except:
                    pass
                    
        return issues
        
    def generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        summary = results['summary']
        
        if summary['syntax_valid'] < summary['total_scripts']:
            failed_syntax = summary['total_scripts'] - summary['syntax_valid']
            recommendations.append(f"Fix {failed_syntax} Python syntax errors before deployment")
            
        if summary['execution_tested'] > 0:
            success_rate = summary['execution_successful'] / summary['execution_tested'] * 100
            if success_rate < 80:
                recommendations.append(f"Improve Python script reliability ({success_rate:.1f}% success rate)")
                
        critical_count = len(results['critical_issues'])
        if critical_count > 0:
            recommendations.append(f"Address {critical_count} critical Python script issues")
            
        # Specific recommendations for common issues
        all_issues = ' '.join(results['critical_issues'])
        if 'import' in all_issues.lower():
            recommendations.append("Review import statements - ensure all dependencies are available in CI")
            
        if 'syntax error' in all_issues.lower():
            recommendations.append("Run local Python syntax validation before committing")
            
        if not recommendations:
            recommendations.append("All Python scripts validated successfully")
            recommendations.append("Consider adding unit tests for complex embedded scripts")
            
        return recommendations


# Mock classes for safe execution
class MockJson:
    """Mock JSON module for safe testing"""
    def dump(self, obj, fp, *args, **kwargs):
        if hasattr(fp, 'write'):
            fp.write(str(obj))
        return True
        
    def dumps(self, obj, *args, **kwargs):
        return str(obj)
        
    def load(self, fp):
        return {'mock': True}
        
    def loads(self, s):
        return {'mock': True}

class MockDateTime:
    """Mock datetime module"""
    class datetime:
        @staticmethod
        def now():
            return type('MockDateTime', (), {
                'isoformat': lambda: '2023-01-01T00:00:00Z'
            })()

class MockOs:
    """Mock os module"""
    @staticmethod
    def makedirs(path, exist_ok=False):
        pass
        
    path = type('path', (), {'exists': lambda p: True})()

class MockSys:
    """Mock sys module"""
    path = ['.']
    
    @staticmethod
    def exit(code):
        raise SystemExit(code)

class MockPath:
    """Mock Path class"""
    def __init__(self, path=''):
        self.path_str = str(path)
        
    def exists(self):
        return True
        
    def mkdir(self, parents=False, exist_ok=False):
        pass
        
    def __str__(self):
        return self.path_str


def main():
    """Main execution function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Test Python scripts in GitHub workflows")
    parser.add_argument("--repo-root", default=".", help="Root directory of repository")
    parser.add_argument("--output", default="tests/workflow-validation/python_test_results.json",
                       help="Output file for test results")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    tester = PythonScriptTester(args.repo_root)
    results = tester.test_all_python_scripts()
    
    # Save results
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
        
    # Print summary
    summary = results['summary']
    
    print("\n" + "="*60)
    print("PYTHON SCRIPT VALIDATION RESULTS")
    print("="*60)
    print(f"Total Workflows: {summary['total_workflows']}")
    print(f"Total Scripts: {summary['total_scripts']}")
    print(f"Syntax Valid: {summary['syntax_valid']}/{summary['total_scripts']}")
    print(f"Execution Tested: {summary['execution_tested']}")
    print(f"Execution Successful: {summary['execution_successful']}/{summary['execution_tested']}")
    
    if summary['execution_tested'] > 0:
        success_rate = summary['execution_successful'] / summary['execution_tested'] * 100
        print(f"Success Rate: {success_rate:.1f}%")
    
    print()
    
    if results['critical_issues']:
        print("CRITICAL ISSUES:")
        for issue in results['critical_issues'][:5]:
            print(f"  - {issue}")
        print()
        
    if results['recommendations']:
        print("RECOMMENDATIONS:")
        for rec in results['recommendations'][:5]:
            print(f"  - {rec}")
        print()
        
    print("="*60)
    
    # Exit code based on results
    has_critical = len(results['critical_issues']) > 0
    syntax_ok = summary['syntax_valid'] == summary['total_scripts']
    
    sys.exit(0 if syntax_ok and not has_critical else 1)


if __name__ == "__main__":
    main()