"""
Comprehensive End-to-End Enterprise Workflow Tests

Tests complete enterprise workflows from start to finish including:
- Full analysis lifecycle with all enterprise features enabled
- Multi-framework compliance validation workflows
- Security analysis and SBOM generation workflows
- Quality metrics and Six Sigma process workflows
- Feature flag controlled rollout scenarios
- Real-world integration scenarios
"""

import pytest
import asyncio
import json
import tempfile
import time
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime, date, timedelta

# Import all enterprise modules
import sys
sys.path.append(str(Path(__file__).parent.parent.parent.parent / 'src'))

from enterprise.flags.feature_flags import flag_manager, FlagStatus
from enterprise.telemetry.six_sigma import SixSigmaTelemetry, QualityLevel
from enterprise.security.sbom_generator import SBOMGenerator, SBOMFormat
from enterprise.compliance.matrix import ComplianceMatrix, ComplianceFramework, ComplianceStatus
from enterprise.integration.analyzer import EnterpriseAnalyzerIntegration


class MockProductionAnalyzer:
    """Mock analyzer that simulates a production code analyzer"""
    
    def __init__(self, *args, **kwargs):
        self.name = "production_analyzer"
        self.analysis_count = 0
        
    async def analyze(self, code_path, options=None):
        """Simulate code analysis"""
        self.analysis_count += 1
        await asyncio.sleep(0.1)  # Simulate analysis time
        
        # Simulate different analysis outcomes
        if "error" in str(code_path):
            raise ValueError("Simulated analysis error")
            
        results = {
            "file_path": str(code_path),
            "lines_of_code": 150,
            "complexity_score": 0.75,
            "security_issues": 0 if "secure" in str(code_path) else 1,
            "quality_score": 0.9 if "high_quality" in str(code_path) else 0.7,
            "analysis_time": 0.1,
            "issues_found": [
                {
                    "type": "style",
                    "severity": "low",
                    "message": "Line too long",
                    "line": 42
                }
            ] if "issues" in str(code_path) else []
        }
        
        return results


class TestCompleteEnterpriseWorkflow:
    """Test complete end-to-end enterprise workflow"""
    
    def setup_method(self):
        """Setup for each test method"""
        self.temp_dir = tempfile.TemporaryDirectory()
        self.project_root = Path(self.temp_dir.name)
        
        # Create test project structure
        (self.project_root / "src").mkdir()
        (self.project_root / "src" / "main.py").write_text('print("Hello World")')
        (self.project_root / "requirements.txt").write_text("requests>=2.25.0\npytest>=6.0.0")
        
        # Clear flags and set up enterprise features
        flag_manager.flags.clear()
        
        # Enable all enterprise features
        flag_manager.create_flag("enterprise_telemetry", "Six Sigma telemetry", status=FlagStatus.ENABLED)
        flag_manager.create_flag("enterprise_security", "Security analysis", status=FlagStatus.ENABLED)
        flag_manager.create_flag("enterprise_compliance", "Compliance checking", status=FlagStatus.ENABLED)
        flag_manager.create_flag("enterprise_metrics", "Quality metrics", status=FlagStatus.ENABLED)
        flag_manager.create_flag("enterprise_sbom", "SBOM generation", status=FlagStatus.ENABLED)
        
    def teardown_method(self):
        """Cleanup after each test"""
        self.temp_dir.cleanup()
        
    @pytest.mark.asyncio
    async def test_full_analysis_lifecycle(self):
        """Test complete analysis lifecycle with all enterprise features"""
        # Initialize enterprise integration
        integration = EnterpriseAnalyzerIntegration(self.project_root)
        
        # Wrap the production analyzer
        integration.wrap_analyzer("production", MockProductionAnalyzer)
        
        # Add workflow hooks
        workflow_events = []
        
        def pre_analysis_hook(analysis_id, *args):
            workflow_events.append(("pre_analysis", analysis_id))
            
        def post_analysis_hook(analysis_id, result):
            workflow_events.append(("post_analysis", analysis_id, result))
            
        def success_hook(analysis_id, result):
            workflow_events.append(("success", analysis_id))
            
        integration.register_hook("pre_analysis", pre_analysis_hook)
        integration.register_hook("post_analysis", post_analysis_hook)
        integration.register_hook("on_success", success_hook)
        
        # Perform comprehensive analysis
        test_files = [\n            "src/secure_code.py",\n            "src/high_quality_code.py",\n            "src/code_with_issues.py",\n            "src/regular_code.py"\n        ]
        
        results = []
        for file_path in test_files:
            result = await integration.analyze_with_enterprise_features(\n                "production",\n                file_path,\n                options={"deep_analysis": True}\n            )
            results.append(result)
            
        # Verify all analyses completed
        assert len(results) == 4
        
        # Verify workflow hooks were called
        assert len(workflow_events) >= 12  # 3 hooks * 4 analyses
        
        # Verify enterprise features were included
        for result in results:
            assert result['analyzer'] == "production"
            assert 'analysis_result' in result
            assert 'quality_metrics' in result
            assert 'security_analysis' in result
            assert 'compliance_status' in result
            
            # Check quality metrics
            quality_metrics = result['quality_metrics']
            assert 'dpmo' in quality_metrics
            assert 'sigma_level' in quality_metrics
            assert quality_metrics['analyzer'] == "production"
            
            # Check compliance status
            compliance_status = result['compliance_status']
            assert 'soc2_compliance' in compliance_status
            assert 'iso27001_compliance' in compliance_status
            
        # Verify telemetry was collected
        telemetry_status = integration.telemetry.export_metrics()
        assert telemetry_status['current_session']['units_processed'] == 4
        assert telemetry_status['current_session']['units_passed'] == 4
        
        # Generate final report
        enterprise_report_file = self.project_root / "enterprise_report.json"
        integration.export_enterprise_report(enterprise_report_file)
        
        assert enterprise_report_file.exists()
        
        with open(enterprise_report_file) as f:
            report_data = json.load(f)
            
        assert 'integration_status' in report_data
        assert 'quality_metrics' in report_data
        assert 'analysis_history' in report_data
        assert len(report_data['analysis_history']) == 4
        
    @pytest.mark.asyncio
    async def test_compliance_validation_workflow(self):
        """Test complete compliance validation workflow"""
        integration = EnterpriseAnalyzerIntegration(self.project_root)
        
        # Get the compliance matrix
        compliance_matrix = integration.compliance
        
        # Verify frameworks were loaded
        assert ComplianceFramework.SOC2_TYPE2 in compliance_matrix.frameworks
        assert ComplianceFramework.ISO27001 in compliance_matrix.frameworks
        assert ComplianceFramework.NIST_CSF in compliance_matrix.frameworks
        
        # Simulate compliance implementation workflow
        soc2_controls = [c for c in compliance_matrix.controls.values() 
                        if c.framework == ComplianceFramework.SOC2_TYPE2][:5]
        
        # Phase 1: Plan and implement controls
        for i, control in enumerate(soc2_controls):
            if i < 2:
                # Implement first 2 controls
                compliance_matrix.update_control_status(
                    control.id,
                    ComplianceStatus.IMPLEMENTED,
                    notes=f"Implemented {control.title}"
                )
                
                # Add evidence
                evidence_file = self.project_root / f"evidence_{control.id.replace('.', '_')}.pdf"
                evidence_file.write_text(f"Evidence for {control.title}")
                compliance_matrix.add_evidence(
                    control.id,
                    evidence_file,
                    f"Implementation evidence for {control.title}"
                )
                
            elif i < 4:
                # Mark as in progress
                compliance_matrix.update_control_status(
                    control.id,
                    ComplianceStatus.IN_PROGRESS,
                    notes="Implementation in progress"
                )
                
        # Phase 2: Test implemented controls
        implemented_controls = [c for c in soc2_controls if c.status == ComplianceStatus.IMPLEMENTED]
        for control in implemented_controls:
            compliance_matrix.update_control_status(
                control.id,
                ComplianceStatus.TESTED,
                notes=f"Testing completed for {control.title}"
            )
            
        # Phase 3: Mark as compliant after successful testing
        tested_controls = [c for c in soc2_controls if c.status == ComplianceStatus.TESTED]
        for control in tested_controls:
            compliance_matrix.update_control_status(
                control.id,
                ComplianceStatus.COMPLIANT,
                notes=f"Control {control.title} is now compliant"
            )
            
        # Generate compliance report
        soc2_report = compliance_matrix.generate_compliance_report(ComplianceFramework.SOC2_TYPE2)
        
        # Verify compliance progress
        assert soc2_report.compliant_controls == 2
        assert soc2_report.in_progress_controls == 2  # IN_PROGRESS controls
        assert soc2_report.overall_status > 0
        
        # Verify recommendations were generated
        assert len(soc2_report.recommendations) > 0
        assert len(soc2_report.next_actions) > 0
        
        # Verify evidence collection
        evidence_dir = compliance_matrix.evidence_directory
        assert evidence_dir.exists()
        
        # Check evidence files exist
        for control in implemented_controls:
            control_evidence_dir = evidence_dir / control.id
            assert control_evidence_dir.exists()
            evidence_files = list(control_evidence_dir.glob("*.pdf"))
            assert len(evidence_files) > 0
            
        # Export compliance matrix
        compliance_export_file = self.project_root / "compliance_matrix.json"
        compliance_matrix.export_compliance_matrix(compliance_export_file)
        
        assert compliance_export_file.exists()
        
        with open(compliance_export_file) as f:
            export_data = json.load(f)
            
        assert "frameworks" in export_data
        assert "controls" in export_data
        assert len(export_data["controls"]) > 0
        
    @pytest.mark.asyncio
    async def test_security_sbom_workflow(self):
        """Test complete security analysis and SBOM generation workflow"""
        # Create realistic project structure
        src_dir = self.project_root / "src"
        src_dir.mkdir(exist_ok=True)
        
        # Create Python files
        (src_dir / "__init__.py").write_text("")
        (src_dir / "main.py").write_text('''
import requests
import json
import os
from pathlib import Path

def main():
    """Main application function"""
    response = requests.get("https://api.example.com/data")
    data = response.json()
    return data

if __name__ == "__main__":
    main()
        ''')
        
        # Create requirements.txt with dependencies
        (self.project_root / "requirements.txt").write_text('''
requests>=2.25.0
click>=8.0.0
pydantic>=1.8.0
        ''')
        
        # Create package.json for mixed project
        package_json = {
            "name": "test-project",
            "version": "1.0.0",
            "dependencies": {
                "express": "^4.17.1",
                "lodash": "^4.17.21"
            },
            "devDependencies": {
                "jest": "^27.0.0"
            }
        }
        
        with open(self.project_root / "package.json", 'w') as f:
            json.dump(package_json, f, indent=2)
            
        # Initialize SBOM generator
        sbom_generator = SBOMGenerator(self.project_root)
        
        # Generate SBOM in multiple formats
        spdx_file = await sbom_generator.generate_sbom(
            format=SBOMFormat.SPDX_JSON,
            output_file=self.project_root / "project.spdx.json"
        )
        
        cyclonedx_file = await sbom_generator.generate_sbom(
            format=SBOMFormat.CYCLONEDX_JSON,
            output_file=self.project_root / "project.cyclonedx.json"
        )
        
        # Verify SBOM files were created
        assert spdx_file.exists()
        assert cyclonedx_file.exists()
        
        # Validate SPDX SBOM content
        with open(spdx_file) as f:
            spdx_data = json.load(f)
            
        assert spdx_data["spdxVersion"] == "SPDX-2.3"
        assert "packages" in spdx_data
        assert "relationships" in spdx_data
        
        # Should have detected dependencies
        packages = spdx_data["packages"]
        package_names = [pkg.get("name") for pkg in packages]
        
        # Should have root package plus dependencies
        assert len(packages) > 1
        assert any("express" in name.lower() for name in package_names if name)
        
        # Validate CycloneDX SBOM content
        with open(cyclonedx_file) as f:
            cyclonedx_data = json.load(f)
            
        assert cyclonedx_data["bomFormat"] == "CycloneDX"
        assert "components" in cyclonedx_data
        assert "metadata" in cyclonedx_data
        
        components = cyclonedx_data["components"]
        assert len(components) > 0
        
        # Test SBOM generator status
        status = sbom_generator.get_status()
        assert status["project_root"] == str(self.project_root)
        assert status["components_count"] > 0
        assert len(status["supported_formats"]) == 4
        
    @pytest.mark.asyncio
    async def test_quality_metrics_workflow(self):
        """Test complete Six Sigma quality metrics workflow"""
        # Initialize telemetry system
        telemetry = SixSigmaTelemetry("quality_workflow")
        
        # Simulate development workflow with quality tracking
        # Phase 1: Initial development with some defects
        development_tasks = [
            ("implement_feature_a", True, 5),   # 5 opportunities, passed
            ("implement_feature_b", False, 8),  # 8 opportunities, failed (defect)
            ("implement_feature_c", True, 3),   # 3 opportunities, passed
            ("fix_bug_1", True, 2),            # 2 opportunities, passed
            ("implement_feature_d", False, 6),  # 6 opportunities, failed (defect)
            ("code_review_fixes", True, 4),     # 4 opportunities, passed
        ]
        
        for task_name, passed, opportunities in development_tasks:
            telemetry.record_unit_processed(passed=passed, opportunities=opportunities)
            if not passed:\n                telemetry.record_defect(f"defect_in_{task_name}", opportunities=1)
                \n        # Generate initial metrics snapshot\n        phase1_metrics = telemetry.generate_metrics_snapshot()\n        \n        assert phase1_metrics.sample_size == 6\n        assert phase1_metrics.defect_count == 4  # 2 failed units + 2 recorded defects\n        assert phase1_metrics.opportunity_count == 30  # Total opportunities: 5+8+3+2+6+4+1+1\n        \n        # Verify quality level determination\n        dpmo = telemetry.calculate_dpmo()\n        quality_level = telemetry.get_quality_level()\n        sigma_level = telemetry.calculate_sigma_level()\n        \n        assert dpmo > 0\n        assert isinstance(quality_level, QualityLevel)\n        assert sigma_level > 0\n        \n        # Phase 2: Process improvement - reduce defects\n        improvement_tasks = [\n            ("refactor_module_a", True, 10),\n            ("add_unit_tests", True, 15),\n            ("implement_feature_e", True, 7),\n            ("code_review", True, 5),\n            ("integration_test", True, 8),\n            ("performance_opt", False, 4),  # One failure for realism\n        ]\n        \n        for task_name, passed, opportunities in improvement_tasks:\n            telemetry.record_unit_processed(passed=passed, opportunities=opportunities)\n            if not passed:\n                telemetry.record_defect(f"defect_in_{task_name}")
                
        # Generate final metrics
        phase2_metrics = telemetry.generate_metrics_snapshot()
        
        # Verify improvement
        assert phase2_metrics.sample_size > phase1_metrics.sample_size
        
        # Calculate improvement metrics
        phase1_dpmo = phase1_metrics.dpmo
        phase2_dpmo = phase2_metrics.dpmo
        
        # DPMO should improve (decrease) or at least not get worse
        assert phase2_dpmo <= phase1_dpmo * 1.1  # Allow for some variance
        
        # Generate trend analysis
        trend_analysis = telemetry.get_trend_analysis(days=1)
        
        assert "error" not in trend_analysis
        assert trend_analysis["sample_count"] == 2  # 2 snapshots
        assert "dpmo" in trend_analysis
        assert "rty" in trend_analysis
        assert "sigma_level" in trend_analysis
        
        # Export complete metrics
        metrics_export = telemetry.export_metrics()
        
        assert metrics_export["process_name"] == "quality_workflow"
        assert len(metrics_export["metrics_history"]) == 2
        assert "current_session" in metrics_export
        
        # Save metrics to file
        metrics_file = self.project_root / "quality_metrics.json"
        with open(metrics_file, 'w') as f:
            json.dump(metrics_export, f, indent=2, default=str)
            
        assert metrics_file.exists()
        
    @pytest.mark.asyncio
    async def test_feature_flag_rollout_workflow(self):
        """Test feature flag controlled rollout workflow"""
        integration = EnterpriseAnalyzerIntegration(self.project_root)
        
        # Create a new feature flag for controlled rollout
        flag_manager.create_flag(
            "new_analysis_engine",
            "New analysis engine rollout",
            status=FlagStatus.ROLLOUT,
            rollout_percentage=25.0,  # Start with 25% rollout
            rollout_strategy="percentage"
        )
        
        # Mock different user scenarios
        test_users = [f"user_{i}" for i in range(20)]
        rollout_results = []
        
        @flag_manager.enterprise_feature("new_analysis_engine", "New analysis engine")
        def new_analysis_method(data, user_id=None):
            """New analysis method behind feature flag"""
            return {"result": f"new_engine_analyzed_{data}", "engine": "v2"}
            
        @new_analysis_method.fallback
        def old_analysis_method(data, user_id=None):
            """Fallback to old analysis method"""
            return {"result": f"old_engine_analyzed_{data}", "engine": "v1"}
            
        # Test initial rollout (25%)
        for user in test_users:
            result = new_analysis_method("test_data", user_id=user)
            rollout_results.append((user, result["engine"]))
            
        # Verify roughly 25% got new engine (allow for hash variance)
        new_engine_count = len([r for r in rollout_results if r[1] == "v2"])
        rollout_percentage = (new_engine_count / len(test_users)) * 100
        
        assert 10 <= rollout_percentage <= 40  # Allow for hash-based variance
        
        # Phase 2: Increase rollout to 75%
        flag_manager.update_flag("new_analysis_engine", rollout_percentage=75.0)
        
        phase2_results = []
        for user in test_users:
            result = new_analysis_method("test_data", user_id=user)
            phase2_results.append((user, result["engine"]))
            
        new_engine_count_phase2 = len([r for r in phase2_results if r[1] == "v2"])
        rollout_percentage_phase2 = (new_engine_count_phase2 / len(test_users)) * 100
        
        # Should have higher rollout percentage
        assert rollout_percentage_phase2 > rollout_percentage
        assert rollout_percentage_phase2 >= 50  # Should be closer to 75%
        
        # Phase 3: Full rollout
        flag_manager.update_flag("new_analysis_engine", status=FlagStatus.ENABLED)
        
        phase3_results = []
        for user in test_users:
            result = new_analysis_method("test_data", user_id=user)
            phase3_results.append((user, result["engine"]))
            
        # All users should get new engine
        new_engine_count_phase3 = len([r for r in phase3_results if r[1] == "v2"])
        assert new_engine_count_phase3 == len(test_users)
        
        # Verify flag metrics
        flag_metrics = flag_manager.get_metrics_summary()
        
        assert flag_metrics["total_flags"] > 0
        assert "new_analysis_engine" in flag_metrics["flag_details"]
        
        flag_detail = flag_metrics["flag_details"]["new_analysis_engine"]
        assert flag_detail["status"] == "enabled"
        assert flag_detail["total_calls"] > 0
        
    @pytest.mark.asyncio
    async def test_error_recovery_workflow(self):
        """Test complete error recovery and resilience workflow"""
        integration = EnterpriseAnalyzerIntegration(self.project_root)
        
        # Create analyzer that fails sometimes
        class UnreliableAnalyzer:
            def __init__(self):
                self.call_count = 0
                
            async def analyze(self, data):
                self.call_count += 1
                # Fail every 3rd call
                if self.call_count % 3 == 0:
                    raise Exception(f"Simulated failure #{self.call_count}")
                return {"result": f"success_{data}", "call": self.call_count}
                
        # Wrap unreliable analyzer
        integration.wrap_analyzer("unreliable", UnreliableAnalyzer)
        
        # Track error recovery
        error_recovery_log = []
        
        def error_recovery_hook(analysis_id, error):
            error_recovery_log.append({
                "analysis_id": analysis_id,
                "error": str(error),
                "timestamp": datetime.now()
            })
            
        integration.register_hook("on_error", error_recovery_hook)
        
        # Run multiple analyses - some will fail
        results = []
        errors = []
        
        wrapped_class = integration.wrapped_analyzers["unreliable"]
        
        for i in range(10):
            instance = wrapped_class()
            try:
                result = await instance.analyze(f"data_{i}")
                results.append(result)
            except Exception as e:
                errors.append((i, str(e)))
                
        # Verify error handling
        assert len(results) > 0  # Some analyses succeeded
        assert len(errors) > 0   # Some analyses failed
        assert len(error_recovery_log) == len(errors)
        
        # Verify telemetry recorded both successes and failures
        telemetry_data = integration.telemetry.export_metrics()
        session_data = telemetry_data["current_session"]
        
        assert session_data["units_processed"] == 10
        assert session_data["units_passed"] == len(results)
        assert session_data["defects"] == len(errors)
        
        # Verify quality metrics reflect the failures
        quality_metrics = integration.telemetry.generate_metrics_snapshot()
        
        assert quality_metrics.dpmo > 0  # Should have defects
        assert quality_metrics.rty < 100  # Should have some failures
        
        # Verify analysis history includes both successes and failures
        analysis_history = integration.analysis_history
        successful_analyses = [a for a in analysis_history if a["success"]]
        failed_analyses = [a for a in analysis_history if not a["success"]]
        
        assert len(successful_analyses) == len(results)
        assert len(failed_analyses) == len(errors)
        
        # Generate resilience report
        integration_status = integration.get_integration_status()
        
        assert integration_status["total_analyses"] == 10
        assert integration_status["successful_analyses"] == len(results)
        assert integration_status["failed_analyses"] == len(errors)
        assert integration_status["average_analysis_time"] > 0


class TestRealWorldIntegrationScenarios:
    """Test real-world enterprise integration scenarios"""
    
    def setup_method(self):
        """Setup for each test method"""
        self.temp_dir = tempfile.TemporaryDirectory()
        self.project_root = Path(self.temp_dir.name)
        
        # Create realistic project structure
        self.create_realistic_project_structure()
        
        # Setup enterprise features with realistic configuration
        flag_manager.flags.clear()
        self.setup_enterprise_configuration()
        
    def teardown_method(self):
        """Cleanup after each test"""
        self.temp_dir.cleanup()
        
    def create_realistic_project_structure(self):
        """Create a realistic project structure for testing"""
        # Python project structure
        (self.project_root / "src" / "myapp").mkdir(parents=True)
        (self.project_root / "src" / "myapp" / "__init__.py").write_text("")
        (self.project_root / "src" / "myapp" / "main.py").write_text("""
import os
import json
import logging
from typing import List, Dict, Optional
from dataclasses import dataclass
import requests

@dataclass
class User:
    id: int
    name: str
    email: str

class UserService:
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.logger = logging.getLogger(__name__)
        
    def get_users(self) -> List[User]:
        \"\"\"Fetch all users from API\"\"\"
        try:
            response = requests.get(f"{self.api_url}/users")
            response.raise_for_status()
            data = response.json()
            return [User(**user_data) for user_data in data]
        except Exception as e:
            self.logger.error(f"Failed to fetch users: {e}")
            return []
            
    def create_user(self, name: str, email: str) -> Optional[User]:
        \"\"\"Create a new user\"\"\"
        payload = {"name": name, "email": email}
        try:
            response = requests.post(f"{self.api_url}/users", json=payload)
            response.raise_for_status()
            return User(**response.json())
        except Exception as e:
            self.logger.error(f"Failed to create user: {e}")
            return None
""")
        
        # Configuration files
        (self.project_root / "requirements.txt").write_text("""
requests>=2.25.0
dataclasses-json>=0.5.4
pytest>=6.2.0
pytest-cov>=2.12.0
black>=21.0.0
flake8>=3.9.0
mypy>=0.812
""")
        
        (self.project_root / "setup.py").write_text("""
from setuptools import setup, find_packages

setup(
    name="myapp",
    version="1.0.0",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "requests>=2.25.0",
        "dataclasses-json>=0.5.4",
    ],
    extras_require={
        "dev": [
            "pytest>=6.2.0",
            "pytest-cov>=2.12.0",
            "black>=21.0.0",
            "flake8>=3.9.0",
            "mypy>=0.812",
        ]
    },
)
""")
        
        # Test files
        (self.project_root / "tests").mkdir()
        (self.project_root / "tests" / "__init__.py").write_text("")
        (self.project_root / "tests" / "test_user_service.py").write_text("""
import pytest
from unittest.mock import Mock, patch
from myapp.main import UserService, User

class TestUserService:
    def test_get_users_success(self):
        service = UserService("http://api.example.com")
        
        with patch('requests.get') as mock_get:
            mock_response = Mock()
            mock_response.json.return_value = [
                {"id": 1, "name": "John", "email": "john@example.com"}
            ]
            mock_response.raise_for_status.return_value = None
            mock_get.return_value = mock_response
            
            users = service.get_users()
            
            assert len(users) == 1
            assert users[0].name == "John"
            
    def test_create_user_success(self):
        service = UserService("http://api.example.com")
        
        with patch('requests.post') as mock_post:
            mock_response = Mock()
            mock_response.json.return_value = {
                "id": 1, "name": "Jane", "email": "jane@example.com"
            }
            mock_response.raise_for_status.return_value = None
            mock_post.return_value = mock_response
            
            user = service.create_user("Jane", "jane@example.com")
            
            assert user is not None
            assert user.name == "Jane"
""")
        
        # Configuration files
        (self.project_root / "pyproject.toml").write_text("""
[tool.black]
line-length = 88
target-version = ['py38']

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "--cov=myapp --cov-report=html --cov-report=term"
""")
        
    def setup_enterprise_configuration(self):
        """Setup enterprise configuration with feature flags"""
        # Progressive rollout flags
        flag_manager.create_flag(
            "enhanced_security_scanning",
            "Enhanced security vulnerability scanning",
            status=FlagStatus.ROLLOUT,
            rollout_percentage=50.0
        )
        
        flag_manager.create_flag(
            "detailed_compliance_reporting",
            "Detailed compliance reporting with evidence",
            status=FlagStatus.ENABLED
        )
        
        flag_manager.create_flag(
            "advanced_quality_metrics",
            "Advanced Six Sigma quality metrics",
            status=FlagStatus.ENABLED
        )
        
        flag_manager.create_flag(
            "automated_sbom_generation",
            "Automated SBOM generation in CI/CD",
            status=FlagStatus.BETA,
            rollout_percentage=100.0
        )
        
        # Feature flags for A/B testing
        flag_manager.create_flag(
            "ml_powered_analysis",
            "ML-powered code analysis",
            status=FlagStatus.AB_TEST
        )
        
    @pytest.mark.asyncio
    async def test_enterprise_ci_cd_pipeline_simulation(self):
        """Simulate complete enterprise CI/CD pipeline with all features"""
        # Initialize enterprise integration
        integration = EnterpriseAnalyzerIntegration(self.project_root)
        
        # Simulate CI/CD stages
        pipeline_results = {
            "stages": [],
            "overall_success": True,
            "quality_gates_passed": True,
            "enterprise_reports": {}
        }
        
        # Stage 1: Code Analysis
        integration.wrap_analyzer("static_analyzer", MockProductionAnalyzer)
        
        source_files = [
            "src/myapp/main.py",
            "src/myapp/__init__.py",
            "tests/test_user_service.py"
        ]
        
        analysis_results = []
        for source_file in source_files:
            result = await integration.analyze_with_enterprise_features(
                "static_analyzer",
                source_file,
                options={"stage": "ci_analysis"}
            )
            analysis_results.append(result)
            
        pipeline_results["stages"].append({
            "name": "code_analysis",
            "success": True,
            "files_analyzed": len(source_files),
            "issues_found": sum(len(r["analysis_result"].get("issues_found", [])) for r in analysis_results)
        })
        
        # Stage 2: Security Analysis & SBOM Generation
        if flag_manager.is_enabled("automated_sbom_generation"):
            sbom_generator = SBOMGenerator(self.project_root)
            
            # Generate SBOM
            sbom_file = await sbom_generator.generate_sbom(
                format=SBOMFormat.SPDX_JSON,
                output_file=self.project_root / "ci_sbom.json"
            )
            
            pipeline_results["stages"].append({
                "name": "sbom_generation",
                "success": sbom_file.exists(),
                "sbom_file": str(sbom_file),
                "components_found": sbom_generator.get_status()["components_count"]
            })
            
        # Stage 3: Compliance Validation
        if flag_manager.is_enabled("detailed_compliance_reporting"):
            compliance_matrix = integration.compliance
            
            # Simulate compliance checks for CI/CD
            ci_controls = []
            for framework in [ComplianceFramework.SOC2_TYPE2, ComplianceFramework.ISO27001]:
                framework_controls = [
                    c for c in compliance_matrix.controls.values()
                    if c.framework == framework
                ][:3]  # Check first 3 controls per framework
                
                for control in framework_controls:
                    # Simulate automated compliance check
                    compliance_matrix.update_control_status(
                        control.id,
                        ComplianceStatus.TESTED,
                        notes=f"Automated CI/CD compliance check - {datetime.now()}"
                    )
                    ci_controls.append(control.id)
                    
            pipeline_results["stages"].append({
                "name": "compliance_validation",
                "success": True,
                "controls_checked": len(ci_controls),
                "frameworks_validated": ["soc2-type2", "iso27001"]
            })
            
        # Stage 4: Quality Gates
        if flag_manager.is_enabled("advanced_quality_metrics"):
            # Generate quality metrics snapshot
            quality_metrics = integration.telemetry.generate_metrics_snapshot()
            
            # Define quality gates
            quality_gates = {
                "sigma_level": {"threshold": 3.0, "actual": quality_metrics.sigma_level},
                "dpmo": {"threshold": 50000, "actual": quality_metrics.dpmo},
                "rty": {"threshold": 80.0, "actual": quality_metrics.rty}
            }
            
            gates_passed = all(
                gate["actual"] >= gate["threshold"] if gate_name in ["sigma_level", "rty"]
                else gate["actual"] <= gate["threshold"]
                for gate_name, gate in quality_gates.items()
            )
            
            pipeline_results["quality_gates_passed"] = gates_passed
            pipeline_results["stages"].append({
                "name": "quality_gates",
                "success": gates_passed,
                "gates": quality_gates,
                "quality_level": quality_metrics.quality_level.name if quality_metrics.quality_level else None
            })
            
        # Stage 5: Generate Enterprise Reports
        enterprise_report_file = self.project_root / "ci_enterprise_report.json"
        integration.export_enterprise_report(enterprise_report_file)
        
        pipeline_results["enterprise_reports"]["integration_report"] = str(enterprise_report_file)
        
        # Export compliance matrix for auditing
        compliance_export_file = self.project_root / "ci_compliance_matrix.json"
        integration.compliance.export_compliance_matrix(compliance_export_file)
        
        pipeline_results["enterprise_reports"]["compliance_matrix"] = str(compliance_export_file)
        
        # Final pipeline status
        pipeline_results["overall_success"] = (
            all(stage["success"] for stage in pipeline_results["stages"]) and
            pipeline_results["quality_gates_passed"]
        )
        
        # Assertions for CI/CD pipeline
        assert pipeline_results["overall_success"] is True
        assert len(pipeline_results["stages"]) >= 3  # At least analysis, compliance, quality
        
        # Verify all reports were generated
        for report_file in pipeline_results["enterprise_reports"].values():
            assert Path(report_file).exists()
            
        # Verify enterprise features were utilized
        with open(enterprise_report_file) as f:
            integration_report = json.load(f)
            
        assert integration_report["integration_status"]["total_analyses"] >= 3
        assert "quality_metrics" in integration_report
        assert "compliance_coverage" in integration_report
        
    @pytest.mark.asyncio
    async def test_multi_team_enterprise_workflow(self):
        """Test enterprise workflow across multiple development teams"""
        # Simulate different teams with different configurations
        teams = {
            "backend_team": {
                "features": ["enhanced_security_scanning", "detailed_compliance_reporting"],
                "analyzers": ["security_analyzer", "api_analyzer"],
                "files": ["src/myapp/main.py"]
            },
            "frontend_team": {
                "features": ["advanced_quality_metrics", "ml_powered_analysis"],
                "analyzers": ["ui_analyzer", "accessibility_analyzer"],
                "files": ["frontend/app.js", "frontend/styles.css"]  # Simulated
            },
            "qa_team": {
                "features": ["automated_sbom_generation", "detailed_compliance_reporting"],
                "analyzers": ["test_analyzer", "integration_analyzer"],
                "files": ["tests/test_user_service.py"]
            }
        }
        
        team_results = {}
        
        for team_name, team_config in teams.items():
            # Create team-specific integration
            team_integration = EnterpriseAnalyzerIntegration(
                self.project_root / team_name
            )
            
            # Configure team-specific analyzers
            for analyzer_name in team_config["analyzers"]:
                team_integration.wrap_analyzer(analyzer_name, MockProductionAnalyzer)
                
            # Run team-specific analyses
            team_analyses = []
            for file_path in team_config["files"]:
                if Path(self.project_root / file_path).exists() or team_name != "frontend_team":
                    # Use first analyzer for the team
                    result = await team_integration.analyze_with_enterprise_features(
                        team_config["analyzers"][0],
                        file_path,
                        options={"team": team_name}
                    )
                    team_analyses.append(result)
                    
            # Generate team report
            team_report_file = self.project_root / f"{team_name}_report.json"
            team_integration.export_enterprise_report(team_report_file)
            
            team_results[team_name] = {
                "analyses_completed": len(team_analyses),
                "report_file": team_report_file,
                "integration_status": team_integration.get_integration_status()
            }
            
        # Verify each team completed their work
        for team_name, results in team_results.items():
            assert results["analyses_completed"] > 0
            assert results["report_file"].exists()
            assert results["integration_status"]["total_analyses"] > 0
            
        # Aggregate results across teams
        total_analyses = sum(r["analyses_completed"] for r in team_results.values())
        total_files_processed = sum(
            r["integration_status"]["total_analyses"]
            for r in team_results.values()
        )
        
        assert total_analyses >= 3  # At least one analysis per team
        assert total_files_processed >= 3
        
        # Generate consolidated enterprise report
        consolidated_integration = EnterpriseAnalyzerIntegration(self.project_root)
        
        # Simulate consolidated analysis for executive reporting
        executive_summary = {
            "report_date": datetime.now().isoformat(),
            "teams_participating": len(teams),
            "total_analyses": total_analyses,
            "total_files_processed": total_files_processed,
            "enterprise_features_adoption": {
                feature: sum(
                    1 for team_config in teams.values()
                    if feature in team_config["features"]
                )
                for feature in [
                    "enhanced_security_scanning",
                    "detailed_compliance_reporting", 
                    "advanced_quality_metrics",
                    "automated_sbom_generation",
                    "ml_powered_analysis"
                ]
            },
            "compliance_status": consolidated_integration.compliance.get_framework_coverage(),
            "quality_metrics": consolidated_integration.telemetry.generate_metrics_snapshot().__dict__
        }
        
        # Save executive summary
        executive_file = self.project_root / "executive_summary.json"
        with open(executive_file, 'w') as f:
            json.dump(executive_summary, f, indent=2, default=str)
            
        assert executive_file.exists()
        
        # Verify executive summary content
        with open(executive_file) as f:
            summary_data = json.load(f)
            
        assert summary_data["teams_participating"] == 3
        assert summary_data["total_analyses"] == total_analyses
        assert "enterprise_features_adoption" in summary_data
        assert "compliance_status" in summary_data


if __name__ == "__main__":
    pytest.main([__file__, "-v"])