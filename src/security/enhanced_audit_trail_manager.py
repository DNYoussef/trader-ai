"""
Enhanced DFARS Audit Trail Manager
Comprehensive audit trail system with SHA-256 integrity verification and tamper detection.
"""

import json
import time
import hashlib
import hmac
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import asyncio
import threading
from queue import Queue, Empty
import gzip
import struct
from concurrent.futures import ThreadPoolExecutor
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
import secrets

from .fips_crypto_module import FIPSCryptoModule

logger = logging.getLogger(__name__)


class AuditEventType(Enum):
    """Extended audit event types for comprehensive logging."""
    # Authentication events
    USER_LOGIN = "user_login"
    USER_LOGOUT = "user_logout"
    LOGIN_FAILED = "login_failed"
    PASSWORD_CHANGED = "password_changed"
    ACCOUNT_LOCKED = "account_locked"

    # Authorization events
    ACCESS_GRANTED = "access_granted"
    ACCESS_DENIED = "access_denied"
    PRIVILEGE_ESCALATION = "privilege_escalation"
    PERMISSION_CHANGED = "permission_changed"

    # Data access events
    DATA_ACCESS = "data_access"
    DATA_MODIFICATION = "data_modification"
    DATA_DELETION = "data_deletion"
    DATA_EXPORT = "data_export"
    DATA_CLASSIFICATION_CHANGE = "data_classification_change"

    # System events
    SYSTEM_START = "system_start"
    SYSTEM_SHUTDOWN = "system_shutdown"
    SERVICE_START = "service_start"
    SERVICE_STOP = "service_stop"
    CONFIGURATION_CHANGE = "configuration_change"

    # Security events
    SECURITY_INCIDENT = "security_incident"
    SECURITY_ALERT = "security_alert"
    MALWARE_DETECTED = "malware_detected"
    INTRUSION_DETECTED = "intrusion_detected"
    VULNERABILITY_DETECTED = "vulnerability_detected"

    # Compliance events
    COMPLIANCE_CHECK = "compliance_check"
    COMPLIANCE_VIOLATION = "compliance_violation"
    AUDIT_LOG_ACCESS = "audit_log_access"
    AUDIT_LOG_BACKUP = "audit_log_backup"
    AUDIT_LOG_RESTORE = "audit_log_restore"

    # Cryptographic events
    CRYPTO_OPERATION = "crypto_operation"
    KEY_GENERATION = "key_generation"
    KEY_ROTATION = "key_rotation"
    CERTIFICATE_ISSUED = "certificate_issued"
    CERTIFICATE_REVOKED = "certificate_revoked"

    # Administrative events
    ADMIN_ACTION = "admin_action"
    POLICY_CHANGE = "policy_change"
    USER_CREATED = "user_created"
    USER_DELETED = "user_deleted"
    ROLE_ASSIGNED = "role_assigned"


class SeverityLevel(Enum):
    """Audit event severity levels."""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class IntegrityStatus(Enum):
    """Audit trail integrity status."""
    VALID = "valid"
    COMPROMISED = "compromised"
    CORRUPTED = "corrupted"
    MISSING = "missing"


@dataclass
class AuditEvent:
    """Enhanced audit event with integrity protection."""
    event_id: str
    timestamp: float
    event_type: AuditEventType
    severity: SeverityLevel
    user_id: Optional[str]
    session_id: Optional[str]
    source_ip: Optional[str]
    user_agent: Optional[str]
    resource: Optional[str]
    action: str
    description: str
    details: Dict[str, Any]
    source_system: str
    correlation_id: Optional[str]

    # Integrity protection fields
    previous_hash: Optional[str]
    content_hash: str
    integrity_signature: Optional[str]
    chain_sequence: int


@dataclass
class AuditChain:
    """Audit chain for integrity verification."""
    chain_id: str
    start_timestamp: float
    end_timestamp: Optional[float]
    event_count: int
    chain_hash: str
    integrity_key_id: str
    signature: Optional[str]
    status: IntegrityStatus


class EnhancedDFARSAuditTrailManager:
    """
    Enhanced DFARS audit trail manager with comprehensive integrity protection,
    tamper detection, and compliance reporting capabilities.
    """

    # DFARS audit retention requirements
    DFARS_RETENTION_DAYS = 2555  # 7 years
    INTEGRITY_CHECK_INTERVAL = 3600  # 1 hour
    BACKUP_INTERVAL = 86400  # 24 hours

    def __init__(self, storage_path: str = ".claude/.artifacts/enhanced_audit"):
        """Initialize enhanced DFARS audit trail manager."""
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)

        # Initialize cryptographic module for integrity protection
        self.crypto_module = FIPSCryptoModule()

        # Audit event storage
        self.audit_buffer = Queue(maxsize=10000)
        self.current_chain: Optional[AuditChain] = None
        self.chain_history: List[AuditChain] = []

        # Integrity protection
        self.integrity_key = self._initialize_integrity_key()
        self.chain_sequence = 0
        self.last_event_hash = None

        # Background processing
        self.processor_active = False
        self.processor_thread: Optional[threading.Thread] = None
        self.executor = ThreadPoolExecutor(max_workers=2)

        # Performance metrics
        self.event_counters = {
            "total_events": 0,
            "events_processed": 0,
            "integrity_checks": 0,
            "integrity_failures": 0,
            "chain_violations": 0
        }

        # Storage configuration
        self.max_events_per_file = 10000
        self.compression_enabled = True
        self.encryption_enabled = True

        # Load existing state
        self._load_existing_state()

        # Start background processor
        self.start_processor()

        logger.info("Enhanced DFARS Audit Trail Manager initialized")

    def _initialize_integrity_key(self) -> bytes:
        """Initialize or load integrity protection key."""
        key_file = self.storage_path / "integrity.key"

        if key_file.exists():
            try:
                with open(key_file, 'rb') as f:
                    return f.read()
            except Exception as e:
                logger.error(f"Failed to load integrity key: {e}")

        # Generate new integrity key
        integrity_key = secrets.token_bytes(32)  # 256-bit key

        try:
            with open(key_file, 'wb') as f:
                f.write(integrity_key)
            # Secure file permissions
            key_file.chmod(0o600)
        except Exception as e:
            logger.error(f"Failed to save integrity key: {e}")

        return integrity_key

    def _load_existing_state(self):
        """Load existing audit trail state."""
        state_file = self.storage_path / "audit_state.json"

        if state_file.exists():
            try:
                with open(state_file, 'r') as f:
                    state_data = json.load(f)

                self.chain_sequence = state_data.get("chain_sequence", 0)
                self.last_event_hash = state_data.get("last_event_hash")
                self.event_counters.update(state_data.get("event_counters", {}))

                # Load chain history
                for chain_data in state_data.get("chain_history", []):
                    chain = AuditChain(
                        chain_id=chain_data["chain_id"],
                        start_timestamp=chain_data["start_timestamp"],
                        end_timestamp=chain_data.get("end_timestamp"),
                        event_count=chain_data["event_count"],
                        chain_hash=chain_data["chain_hash"],
                        integrity_key_id=chain_data["integrity_key_id"],
                        signature=chain_data.get("signature"),
                        status=IntegrityStatus(chain_data["status"])
                    )
                    self.chain_history.append(chain)

                logger.info(f"Loaded audit state: {self.event_counters['total_events']} total events")

            except Exception as e:
                logger.error(f"Failed to load audit state: {e}")

    def _save_state(self):
        """Save current audit trail state."""
        state_data = {
            "chain_sequence": self.chain_sequence,
            "last_event_hash": self.last_event_hash,
            "event_counters": self.event_counters,
            "chain_history": [
                {
                    "chain_id": chain.chain_id,
                    "start_timestamp": chain.start_timestamp,
                    "end_timestamp": chain.end_timestamp,
                    "event_count": chain.event_count,
                    "chain_hash": chain.chain_hash,
                    "integrity_key_id": chain.integrity_key_id,
                    "signature": chain.signature,
                    "status": chain.status.value
                }
                for chain in self.chain_history
            ]
        }

        state_file = self.storage_path / "audit_state.json"
        try:
            with open(state_file, 'w') as f:
                json.dump(state_data, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save audit state: {e}")

    def start_processor(self):
        """Start background audit event processor."""
        if self.processor_active:
            return

        self.processor_active = True
        self.processor_thread = threading.Thread(
            target=self._audit_processor_loop,
            daemon=True,
            name="AuditProcessor"
        )
        self.processor_thread.start()

        # Start integrity checker
        asyncio.create_task(self._integrity_check_loop())

        logger.info("Started audit event processor")

    def stop_processor(self):
        """Stop background audit event processor."""
        self.processor_active = False

        if self.processor_thread and self.processor_thread.is_alive():
            self.processor_thread.join(timeout=5.0)

        # Process remaining events
        self._process_remaining_events()

        # Save final state
        self._save_state()

        logger.info("Stopped audit event processor")

    def log_audit_event(self, event_type: AuditEventType, severity: SeverityLevel,
                       action: str, description: str, details: Optional[Dict[str, Any]] = None,
                       user_id: Optional[str] = None, session_id: Optional[str] = None,
                       source_ip: Optional[str] = None, user_agent: Optional[str] = None,
                       resource: Optional[str] = None, source_system: str = "system",
                       correlation_id: Optional[str] = None) -> str:
        """Log comprehensive audit event with integrity protection."""
        event_id = self._generate_event_id()
        timestamp = time.time()

        # Create audit event
        event = AuditEvent(
            event_id=event_id,
            timestamp=timestamp,
            event_type=event_type,
            severity=severity,
            user_id=user_id,
            session_id=session_id,
            source_ip=source_ip,
            user_agent=user_agent,
            resource=resource,
            action=action,
            description=description,
            details=details or {},
            source_system=source_system,
            correlation_id=correlation_id,
            previous_hash=self.last_event_hash,
            content_hash="",  # Will be calculated
            integrity_signature=None,  # Will be calculated
            chain_sequence=self.chain_sequence + 1
        )

        # Calculate content hash
        event.content_hash = self._calculate_content_hash(event)

        # Calculate integrity signature
        event.integrity_signature = self._calculate_integrity_signature(event)

        # Update sequence and hash chain
        self.chain_sequence += 1
        self.last_event_hash = event.content_hash

        # Queue event for processing
        try:
            self.audit_buffer.put_nowait(event)
            self.event_counters["total_events"] += 1
        except Exception as e:
            logger.error(f"Failed to queue audit event: {e}")

        return event_id

    def _generate_event_id(self) -> str:
        """Generate unique event ID."""
        return f"ae_{int(time.time() * 1000000)}_{secrets.token_hex(8)}"

    def _calculate_content_hash(self, event: AuditEvent) -> str:
        """Calculate SHA-256 hash of event content."""
        # Create deterministic content string
        content_parts = [
            event.event_id,
            str(event.timestamp),
            event.event_type.value,
            event.severity.value,
            event.user_id or "",
            event.session_id or "",
            event.source_ip or "",
            event.user_agent or "",
            event.resource or "",
            event.action,
            event.description,
            json.dumps(event.details, sort_keys=True),
            event.source_system,
            event.correlation_id or "",
            event.previous_hash or "",
            str(event.chain_sequence)
        ]

        content_string = "|".join(content_parts)
        return hashlib.sha256(content_string.encode('utf-8')).hexdigest()

    def _calculate_integrity_signature(self, event: AuditEvent) -> str:
        """Calculate HMAC signature for integrity protection."""
        signature_data = f"{event.content_hash}|{event.chain_sequence}|{event.timestamp}"
        return hmac.new(
            self.integrity_key,
            signature_data.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()

    def _audit_processor_loop(self):
        """Main audit event processing loop."""
        batch_size = 100
        batch_timeout = 1.0
        events_batch = []

        while self.processor_active:
            try:
                # Collect events for batch processing
                batch_start = time.time()

                while len(events_batch) < batch_size and (time.time() - batch_start) < batch_timeout:
                    try:
                        event = self.audit_buffer.get(timeout=0.1)
                        events_batch.append(event)
                        self.audit_buffer.task_done()
                    except Empty:
                        break

                if events_batch:
                    self._process_events_batch(events_batch)
                    events_batch.clear()

                # Periodic maintenance
                if int(time.time()) % 300 == 0:  # Every 5 minutes
                    self._perform_maintenance()

            except Exception as e:
                logger.error(f"Audit processor error: {e}")
                time.sleep(1.0)

    def _process_events_batch(self, events: List[AuditEvent]):
        """Process batch of audit events."""
        try:
            # Initialize chain if needed
            if not self.current_chain:
                self._start_new_chain()

            # Process each event
            for event in events:
                self._process_single_event(event)

            # Check if chain should be rotated
            if self.current_chain and self.current_chain.event_count >= self.max_events_per_file:
                self._rotate_chain()

            self.event_counters["events_processed"] += len(events)

        except Exception as e:
            logger.error(f"Failed to process events batch: {e}")

    def _start_new_chain(self):
        """Start new audit chain."""
        chain_id = f"chain_{int(time.time())}_{secrets.token_hex(8)}"

        self.current_chain = AuditChain(
            chain_id=chain_id,
            start_timestamp=time.time(),
            end_timestamp=None,
            event_count=0,
            chain_hash="",
            integrity_key_id=hashlib.sha256(self.integrity_key).hexdigest()[:16],
            signature=None,
            status=IntegrityStatus.VALID
        )

        logger.info(f"Started new audit chain: {chain_id}")

    def _process_single_event(self, event: AuditEvent):
        """Process individual audit event."""
        # Verify event integrity
        if not self._verify_event_integrity(event):
            self.event_counters["integrity_failures"] += 1
            logger.error(f"Integrity verification failed for event {event.event_id}")
            return

        # Write event to storage
        self._write_event_to_storage(event)

        # Update chain
        if self.current_chain:
            self.current_chain.event_count += 1
            self.current_chain.chain_hash = self._update_chain_hash(
                self.current_chain.chain_hash, event.content_hash
            )

    def _verify_event_integrity(self, event: AuditEvent) -> bool:
        """Verify event integrity signature."""
        expected_signature = self._calculate_integrity_signature(event)
        return hmac.compare_digest(event.integrity_signature or "", expected_signature)

    def _write_event_to_storage(self, event: AuditEvent):
        """Write audit event to persistent storage."""
        # Determine storage file
        date_str = datetime.fromtimestamp(event.timestamp).strftime("%Y%m%d")
        storage_file = self.storage_path / f"audit_events_{date_str}.jsonl"

        # Prepare event data
        event_data = {
            "event_id": event.event_id,
            "timestamp": event.timestamp,
            "iso_timestamp": datetime.fromtimestamp(event.timestamp, timezone.utc).isoformat(),
            "event_type": event.event_type.value,
            "severity": event.severity.value,
            "user_id": event.user_id,
            "session_id": event.session_id,
            "source_ip": event.source_ip,
            "user_agent": event.user_agent,
            "resource": event.resource,
            "action": event.action,
            "description": event.description,
            "details": event.details,
            "source_system": event.source_system,
            "correlation_id": event.correlation_id,
            "previous_hash": event.previous_hash,
            "content_hash": event.content_hash,
            "integrity_signature": event.integrity_signature,
            "chain_sequence": event.chain_sequence
        }

        # Write event (with optional encryption and compression)
        try:
            event_json = json.dumps(event_data, separators=(',', ':'))

            if self.encryption_enabled:
                event_json = self._encrypt_event_data(event_json)

            if self.compression_enabled and not self.encryption_enabled:
                event_json = gzip.compress(event_json.encode('utf-8'))
                storage_file = storage_file.with_suffix('.jsonl.gz')

            with open(storage_file, 'ab') as f:
                if isinstance(event_json, bytes):
                    f.write(event_json + b'\n')
                else:
                    f.write(event_json.encode('utf-8') + b'\n')

        except Exception as e:
            logger.error(f"Failed to write event to storage: {e}")

    def _encrypt_event_data(self, event_json: str) -> bytes:
        """Encrypt event data for secure storage."""
        try:
            # Generate symmetric key for this event
            key, key_id = self.crypto_module.generate_symmetric_key("AES-256-GCM")

            # Encrypt event data
            encrypted_data = self.crypto_module.encrypt_data(
                event_json.encode('utf-8'), key, "AES-256-GCM"
            )

            # Store encryption key securely (in production, use key management system)
            key_storage = self.storage_path / "event_keys"
            key_storage.mkdir(exist_ok=True)

            with open(key_storage / f"{key_id}.key", 'wb') as f:
                f.write(key)

            # Return encrypted data with metadata
            encrypted_event = {
                "encrypted": True,
                "key_id": key_id,
                "algorithm": "AES-256-GCM",
                "data": {
                    "ciphertext": encrypted_data["ciphertext"].hex(),
                    "iv": encrypted_data["iv"].hex(),
                    "tag": encrypted_data["tag"].hex()
                }
            }

            return json.dumps(encrypted_event, separators=(',', ':')).encode('utf-8')

        except Exception as e:
            logger.error(f"Failed to encrypt event data: {e}")
            return event_json.encode('utf-8')

    def _update_chain_hash(self, previous_chain_hash: str, event_hash: str) -> str:
        """Update audit chain hash."""
        chain_data = f"{previous_chain_hash}|{event_hash}"
        return hashlib.sha256(chain_data.encode('utf-8')).hexdigest()

    def _rotate_chain(self):
        """Rotate current audit chain."""
        if not self.current_chain:
            return

        # Finalize current chain
        self.current_chain.end_timestamp = time.time()

        # Sign chain for tamper detection
        chain_data = f"{self.current_chain.chain_id}|{self.current_chain.chain_hash}|{self.current_chain.event_count}"
        chain_signature = hmac.new(
            self.integrity_key,
            chain_data.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()

        self.current_chain.signature = chain_signature

        # Store chain metadata
        self._store_chain_metadata(self.current_chain)

        # Add to history
        self.chain_history.append(self.current_chain)

        logger.info(f"Rotated audit chain: {self.current_chain.chain_id} ({self.current_chain.event_count} events)")

        # Start new chain
        self.current_chain = None

    def _store_chain_metadata(self, chain: AuditChain):
        """Store audit chain metadata."""
        metadata_file = self.storage_path / f"chain_metadata_{chain.chain_id}.json"

        metadata = {
            "chain_id": chain.chain_id,
            "start_timestamp": chain.start_timestamp,
            "end_timestamp": chain.end_timestamp,
            "event_count": chain.event_count,
            "chain_hash": chain.chain_hash,
            "integrity_key_id": chain.integrity_key_id,
            "signature": chain.signature,
            "status": chain.status.value
        }

        try:
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to store chain metadata: {e}")

    def _perform_maintenance(self):
        """Perform periodic maintenance tasks."""
        try:
            # Save current state
            self._save_state()

            # Clean up old temporary files
            self._cleanup_old_files()

            # Backup audit logs if needed
            if self._should_backup():
                self._backup_audit_logs()

        except Exception as e:
            logger.error(f"Maintenance error: {e}")

    def _cleanup_old_files(self):
        """Clean up old audit files beyond retention period."""
        cutoff_time = time.time() - (self.DFARS_RETENTION_DAYS * 24 * 3600)

        # Clean up old event files
        for event_file in self.storage_path.glob("audit_events_*.jsonl*"):
            if event_file.stat().st_mtime < cutoff_time:
                try:
                    event_file.unlink()
                    logger.info(f"Cleaned up old audit file: {event_file}")
                except Exception as e:
                    logger.error(f"Failed to clean up {event_file}: {e}")

    def _should_backup(self) -> bool:
        """Check if audit logs should be backed up."""
        backup_file = self.storage_path / "last_backup.timestamp"

        if not backup_file.exists():
            return True

        try:
            with open(backup_file, 'r') as f:
                last_backup = float(f.read().strip())
            return (time.time() - last_backup) >= self.BACKUP_INTERVAL
        except Exception:
            return True

    def _backup_audit_logs(self):
        """Backup audit logs for long-term retention."""
        backup_dir = self.storage_path / "backups"
        backup_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = backup_dir / f"audit_backup_{timestamp}.tar.gz"

        try:
            import tarfile

            with tarfile.open(backup_file, "w:gz") as tar:
                for audit_file in self.storage_path.glob("audit_events_*.jsonl*"):
                    tar.add(audit_file, arcname=audit_file.name)

                for chain_file in self.storage_path.glob("chain_metadata_*.json"):
                    tar.add(chain_file, arcname=chain_file.name)

            # Update backup timestamp
            with open(self.storage_path / "last_backup.timestamp", 'w') as f:
                f.write(str(time.time()))

            logger.info(f"Created audit backup: {backup_file}")

        except Exception as e:
            logger.error(f"Failed to create audit backup: {e}")

    async def _integrity_check_loop(self):
        """Periodic integrity checking loop."""
        while self.processor_active:
            try:
                await asyncio.sleep(self.INTEGRITY_CHECK_INTERVAL)

                if self.processor_active:
                    integrity_result = await self.verify_audit_trail_integrity()
                    self.event_counters["integrity_checks"] += 1

                    if not integrity_result["overall_integrity"]:
                        self.event_counters["integrity_failures"] += integrity_result["failures"]
                        logger.error("Audit trail integrity verification failed")

                        # Log integrity failure
                        self.log_audit_event(
                            event_type=AuditEventType.SECURITY_ALERT,
                            severity=SeverityLevel.CRITICAL,
                            action="integrity_check_failed",
                            description="Audit trail integrity verification failed",
                            details=integrity_result
                        )

            except Exception as e:
                logger.error(f"Integrity check error: {e}")

    def _process_remaining_events(self):
        """Process remaining events in queue during shutdown."""
        remaining_events = []

        while not self.audit_buffer.empty():
            try:
                event = self.audit_buffer.get_nowait()
                remaining_events.append(event)
                self.audit_buffer.task_done()
            except Empty:
                break

        if remaining_events:
            self._process_events_batch(remaining_events)
            logger.info(f"Processed {len(remaining_events)} remaining events during shutdown")

    async def verify_audit_trail_integrity(self, start_time: Optional[float] = None,
                                         end_time: Optional[float] = None) -> Dict[str, Any]:
        """Verify integrity of audit trail."""
        verification_start = time.time()

        result = {
            "verification_timestamp": verification_start,
            "start_time": start_time,
            "end_time": end_time,
            "overall_integrity": True,
            "chains_verified": 0,
            "events_verified": 0,
            "failures": 0,
            "chain_results": [],
            "error_details": []
        }

        try:
            # Verify each audit chain
            chains_to_verify = self.chain_history.copy()
            if self.current_chain:
                chains_to_verify.append(self.current_chain)

            for chain in chains_to_verify:
                if start_time and chain.end_timestamp and chain.end_timestamp < start_time:
                    continue
                if end_time and chain.start_timestamp > end_time:
                    continue

                chain_result = await self._verify_chain_integrity(chain)
                result["chain_results"].append(chain_result)
                result["chains_verified"] += 1

                if not chain_result["integrity_valid"]:
                    result["overall_integrity"] = False
                    result["failures"] += 1

                result["events_verified"] += chain_result["events_verified"]

        except Exception as e:
            result["overall_integrity"] = False
            result["error_details"].append(str(e))
            logger.error(f"Audit trail verification error: {e}")

        result["verification_duration"] = time.time() - verification_start
        return result

    async def _verify_chain_integrity(self, chain: AuditChain) -> Dict[str, Any]:
        """Verify integrity of individual audit chain."""
        result = {
            "chain_id": chain.chain_id,
            "integrity_valid": True,
            "events_verified": 0,
            "hash_chain_valid": True,
            "signature_valid": True,
            "event_integrity_failures": [],
            "error_details": []
        }

        try:
            # Load events for this chain
            events = await self._load_chain_events(chain)
            result["events_verified"] = len(events)

            # Verify event integrity signatures
            for event in events:
                if not self._verify_event_integrity(event):
                    result["integrity_valid"] = False
                    result["event_integrity_failures"].append(event.event_id)

            # Verify hash chain
            if not self._verify_hash_chain(events):
                result["integrity_valid"] = False
                result["hash_chain_valid"] = False

            # Verify chain signature
            if chain.signature:
                chain_data = f"{chain.chain_id}|{chain.chain_hash}|{chain.event_count}"
                expected_signature = hmac.new(
                    self.integrity_key,
                    chain_data.encode('utf-8'),
                    hashlib.sha256
                ).hexdigest()

                if not hmac.compare_digest(chain.signature, expected_signature):
                    result["integrity_valid"] = False
                    result["signature_valid"] = False

        except Exception as e:
            result["integrity_valid"] = False
            result["error_details"].append(str(e))

        return result

    async def _load_chain_events(self, chain: AuditChain) -> List[AuditEvent]:
        """Load events for specific audit chain."""
        # This would load events from storage files
        # For now, return empty list as this is a complex operation
        # requiring parsing stored audit files
        return []

    def _verify_hash_chain(self, events: List[AuditEvent]) -> bool:
        """Verify hash chain continuity."""
        if not events:
            return True

        # Sort events by chain sequence
        sorted_events = sorted(events, key=lambda e: e.chain_sequence)

        for i, event in enumerate(sorted_events):
            if i > 0:
                previous_event = sorted_events[i - 1]
                if event.previous_hash != previous_event.content_hash:
                    return False

            # Verify content hash
            calculated_hash = self._calculate_content_hash(event)
            if calculated_hash != event.content_hash:
                return False

        return True

    def get_audit_statistics(self, days: int = 7) -> Dict[str, Any]:
        """Get audit trail statistics."""
        cutoff_time = time.time() - (days * 24 * 3600)

        stats = {
            "period_days": days,
            "total_events": self.event_counters["total_events"],
            "events_processed": self.event_counters["events_processed"],
            "integrity_checks": self.event_counters["integrity_checks"],
            "integrity_failures": self.event_counters["integrity_failures"],
            "chain_violations": self.event_counters["chain_violations"],
            "active_chains": 1 if self.current_chain else 0,
            "completed_chains": len(self.chain_history),
            "processor_active": self.processor_active,
            "storage_path": str(self.storage_path),
            "retention_days": self.DFARS_RETENTION_DAYS,
            "event_types": {},
            "severity_distribution": {},
            "recent_activity": []
        }

        # Additional statistics would be calculated by analyzing stored events
        # This is a simplified version for demonstration

        return stats

    def search_audit_events(self, query: Dict[str, Any], limit: int = 1000) -> List[Dict[str, Any]]:
        """Search audit events based on criteria."""
        # This would implement full-text search across audit events
        # For now, return empty list as this requires indexing implementation
        return []

    def export_audit_report(self, start_time: float, end_time: float,
                          export_format: str = "json") -> str:
        """Export audit events for compliance reporting."""
        export_id = f"export_{int(time.time())}_{secrets.token_hex(8)}"
        export_file = self.storage_path / f"audit_export_{export_id}.{export_format}"

        # This would implement comprehensive audit log export
        # For demonstration, create minimal export

        export_data = {
            "export_id": export_id,
            "export_timestamp": time.time(),
            "start_time": start_time,
            "end_time": end_time,
            "format": export_format,
            "events": [],  # Would be populated with actual events
            "integrity_verification": {},  # Would include integrity check results
            "metadata": {
                "total_events": self.event_counters["total_events"],
                "retention_policy": f"{self.DFARS_RETENTION_DAYS} days",
                "encryption_enabled": self.encryption_enabled,
                "compression_enabled": self.compression_enabled
            }
        }

        try:
            with open(export_file, 'w') as f:
                json.dump(export_data, f, indent=2)

            logger.info(f"Created audit export: {export_file}")
            return str(export_file)

        except Exception as e:
            logger.error(f"Failed to create audit export: {e}")
            raise

    # Convenience methods for common audit events
    def log_user_authentication(self, user_id: str, success: bool, source_ip: str,
                              user_agent: str, details: Optional[Dict[str, Any]] = None):
        """Log user authentication event."""
        event_type = AuditEventType.USER_LOGIN if success else AuditEventType.LOGIN_FAILED
        severity = SeverityLevel.INFO if success else SeverityLevel.WARNING

        self.log_audit_event(
            event_type=event_type,
            severity=severity,
            action="user_authentication",
            description=f"User authentication {'successful' if success else 'failed'}",
            user_id=user_id,
            source_ip=source_ip,
            user_agent=user_agent,
            details=details
        )

    def log_data_access(self, user_id: str, resource: str, action: str,
                       success: bool, details: Optional[Dict[str, Any]] = None):
        """Log data access event."""
        event_type = AuditEventType.DATA_ACCESS
        severity = SeverityLevel.INFO

        self.log_audit_event(
            event_type=event_type,
            severity=severity,
            action=action,
            description=f"Data access: {action} on {resource}",
            user_id=user_id,
            resource=resource,
            details=details
        )

    def log_security_event(self, event_type: AuditEventType, severity: SeverityLevel,
                          description: str, details: Optional[Dict[str, Any]] = None):
        """Log security-related event."""
        self.log_audit_event(
            event_type=event_type,
            severity=severity,
            action="security_event",
            description=description,
            source_system="security_monitor",
            details=details
        )

    def log_compliance_check(self, check_type: str, result: str,
                           details: Optional[Dict[str, Any]] = None):
        """Log compliance check event."""
        severity = SeverityLevel.INFO if result == "SUCCESS" else SeverityLevel.WARNING

        self.log_audit_event(
            event_type=AuditEventType.COMPLIANCE_CHECK,
            severity=severity,
            action=check_type,
            description=f"Compliance check: {check_type} - {result}",
            source_system="compliance_monitor",
            details=details
        )

    def log_configuration_change(self, change_type: str, component: str,
                               old_value: Any, new_value: Any, change_reason: str,
                               details: Optional[Dict[str, Any]] = None):
        """Log configuration change event."""
        self.log_audit_event(
            event_type=AuditEventType.CONFIGURATION_CHANGE,
            severity=SeverityLevel.WARNING,
            action=change_type,
            description=f"Configuration change in {component}: {change_reason}",
            source_system="configuration_manager",
            details={
                "component": component,
                "old_value": old_value,
                "new_value": new_value,
                "change_reason": change_reason,
                **(details or {})
            }
        )


# Factory function
def create_enhanced_audit_manager(storage_path: str = ".claude/.artifacts/enhanced_audit") -> EnhancedDFARSAuditTrailManager:
    """Create enhanced DFARS audit trail manager."""
    return EnhancedDFARSAuditTrailManager(storage_path)


# Example usage
if __name__ == "__main__":
    async def main():
        # Initialize enhanced audit manager
        audit_manager = create_enhanced_audit_manager()

        print("Enhanced DFARS Audit Trail Manager")
        print("=" * 40)

        # Log some test events
        audit_manager.log_user_authentication(
            user_id="admin",
            success=True,
            source_ip="192.168.1.100",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )

        audit_manager.log_data_access(
            user_id="admin",
            resource="/sensitive/documents/classified.pdf",
            action="read",
            success=True,
            details={"file_size": 1024000, "classification": "confidential"}
        )

        audit_manager.log_security_event(
            event_type=AuditEventType.SECURITY_ALERT,
            severity=SeverityLevel.HIGH,
            description="Suspicious login pattern detected",
            details={"pattern": "multiple_failed_attempts", "count": 5}
        )

        # Wait for processing
        await asyncio.sleep(2)

        # Get statistics
        stats = audit_manager.get_audit_statistics()
        print(f"Total events: {stats['total_events']}")
        print(f"Events processed: {stats['events_processed']}")
        print(f"Integrity checks: {stats['integrity_checks']}")

        # Verify integrity
        integrity_result = await audit_manager.verify_audit_trail_integrity()
        print(f"Audit trail integrity: {'VALID' if integrity_result['overall_integrity'] else 'COMPROMISED'}")

        # Cleanup
        audit_manager.stop_processor()

        return audit_manager

    # Run example
    asyncio.run(main())