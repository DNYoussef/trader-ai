#!/usr/bin/env python3
"""
Integration Specialist Agent Node - Mesh Network Specialist  
Specializes in real-time result ingestion and cross-tool correlation.
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import threading
from concurrent.futures import ThreadPoolExecutor

from ..adapters.base_adapter import LinterResult, LinterViolation
from ..severity-mapping.unified_severity import unified_mapper

@dataclass
class IngestionConfig:
    """Configuration for real-time result ingestion"""
    buffer_size: int = 1000
    flush_interval: float = 5.0
    correlation_window: float = 30.0
    max_concurrent_streams: int = 10
    enable_real_time: bool = True

@dataclass
class CorrelationRule:
    """Rule for correlating violations across tools"""
    rule_id: str
    tools: List[str]
    location_tolerance: int = 0  # Lines of difference allowed
    correlation_type: str  # 'same_issue', 'related_issue', 'complementary'
    confidence_threshold: float = 0.7

class RealTimeStream:
    """Real-time stream for linter result ingestion"""
    
    def __init__(self, stream_id: str, tool_name: str, buffer_size: int = 100):
        self.stream_id = stream_id
        self.tool_name = tool_name
        self.buffer = deque(maxlen=buffer_size)
        self.subscribers: Set[callable] = set()
        self.active = True
        self.last_update = time.time()
        
    def push_result(self, result: LinterResult) -> None:
        """Push new result to the stream"""
        if self.active:
            self.buffer.append({
                'timestamp': time.time(),
                'result': result,
                'tool_name': self.tool_name
            })
            self.last_update = time.time()
            self._notify_subscribers(result)
            
    def _notify_subscribers(self, result: LinterResult) -> None:
        """Notify all subscribers of new result"""
        for subscriber in self.subscribers:
            try:
                subscriber(self.stream_id, result)
            except Exception as e:
                logging.error(f"Error notifying subscriber: {e}")
                
    def subscribe(self, callback: callable) -> None:
        """Subscribe to stream updates"""
        self.subscribers.add(callback)
        
    def unsubscribe(self, callback: callable) -> None:
        """Unsubscribe from stream updates"""
        self.subscribers.discard(callback)
        
    def get_recent_results(self, count: int = 10) -> List[Dict[str, Any]]:
        """Get recent results from buffer"""
        return list(self.buffer)[-count:]

class CorrelationEngine:
    """Engine for correlating violations across different linting tools"""
    
    def __init__(self):
        self.correlation_rules: List[CorrelationRule] = []
        self.violation_cache: Dict[str, List[LinterViolation]] = defaultdict(list)
        self.correlation_results: List[Dict[str, Any]] = []
        self._load_default_rules()
        
    def _load_default_rules(self) -> None:
        """Load default correlation rules"""
        self.correlation_rules = [
            CorrelationRule(
                rule_id="style_consistency",
                tools=["flake8", "ruff"],
                location_tolerance=0,
                correlation_type="same_issue",
                confidence_threshold=0.9
            ),
            CorrelationRule(
                rule_id="import_optimization",
                tools=["flake8", "ruff", "pylint"],
                location_tolerance=5,
                correlation_type="related_issue",
                confidence_threshold=0.8
            ),
            CorrelationRule(
                rule_id="type_safety",
                tools=["mypy", "pylint"],
                location_tolerance=0,
                correlation_type="complementary",
                confidence_threshold=0.7
            ),
            CorrelationRule(
                rule_id="security_analysis",
                tools=["bandit", "pylint"],
                location_tolerance=2,
                correlation_type="complementary",
                confidence_threshold=0.8
            )
        ]
        
    def add_violations(self, tool_name: str, violations: List[LinterViolation]) -> None:
        """Add violations for correlation analysis"""
        self.violation_cache[tool_name] = violations
        
    def correlate_violations(self) -> List[Dict[str, Any]]:
        """Correlate violations across all cached tools"""
        correlations = []
        
        for rule in self.correlation_rules:
            # Get violations from relevant tools
            relevant_violations = {}
            for tool in rule.tools:
                if tool in self.violation_cache:
                    relevant_violations[tool] = self.violation_cache[tool]
                    
            if len(relevant_violations) >= 2:
                rule_correlations = self._apply_correlation_rule(rule, relevant_violations)
                correlations.extend(rule_correlations)
                
        self.correlation_results = correlations
        return correlations
        
    def _apply_correlation_rule(self, rule: CorrelationRule, 
                              violations: Dict[str, List[LinterViolation]]) -> List[Dict[str, Any]]:
        """Apply a specific correlation rule"""
        correlations = []
        tools = list(violations.keys())
        
        # Compare violations between tool pairs
        for i, tool1 in enumerate(tools):
            for j, tool2 in enumerate(tools[i+1:], i+1):
                correlations.extend(
                    self._correlate_tool_pair(rule, tool1, tool2, 
                                            violations[tool1], violations[tool2])
                )
                
        return correlations
        
    def _correlate_tool_pair(self, rule: CorrelationRule, tool1: str, tool2: str,
                           violations1: List[LinterViolation], 
                           violations2: List[LinterViolation]) -> List[Dict[str, Any]]:
        """Correlate violations between two tools"""
        correlations = []
        
        for v1 in violations1:
            for v2 in violations2:
                correlation = self._calculate_correlation(rule, v1, v2)
                if correlation and correlation['confidence'] >= rule.confidence_threshold:
                    correlations.append({
                        'rule_id': rule.rule_id,
                        'correlation_type': rule.correlation_type,
                        'tool1': tool1,
                        'tool2': tool2,
                        'violation1': v1.to_dict(),
                        'violation2': v2.to_dict(),
                        'confidence': correlation['confidence'],
                        'correlation_factors': correlation['factors']
                    })
                    
        return correlations
        
    def _calculate_correlation(self, rule: CorrelationRule, 
                             v1: LinterViolation, v2: LinterViolation) -> Optional[Dict[str, Any]]:
        """Calculate correlation between two violations"""
        factors = {}
        confidence = 0.0
        
        # Location similarity
        if v1.file_path == v2.file_path:
            factors['same_file'] = True
            confidence += 0.3
            
            line_diff = abs(v1.line_number - v2.line_number)
            if line_diff <= rule.location_tolerance:
                factors['location_match'] = True
                confidence += 0.4 * (1.0 - line_diff / max(rule.location_tolerance, 1))
                
        # Message similarity
        message_similarity = self._calculate_message_similarity(v1.message, v2.message)
        if message_similarity > 0.5:
            factors['message_similarity'] = message_similarity
            confidence += 0.2 * message_similarity
            
        # Severity correlation
        if v1.severity == v2.severity:
            factors['same_severity'] = True
            confidence += 0.1
            
        return {
            'confidence': min(confidence, 1.0),
            'factors': factors
        } if confidence > 0 else None
        
    def _calculate_message_similarity(self, msg1: str, msg2: str) -> float:
        """Calculate similarity between violation messages"""
        words1 = set(msg1.lower().split())
        words2 = set(msg2.lower().split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0

class IntegrationSpecialistNode:
    """
    Integration Specialist node for real-time result ingestion and correlation.
    Peer node in mesh topology for linter integration coordination.
    """
    
    def __init__(self, node_id: str = "integration-specialist"):
        self.node_id = node_id
        self.peer_connections = set()
        self.logger = self._setup_logging()
        
        # Real-time streaming components
        self.streams: Dict[str, RealTimeStream] = {}
        self.ingestion_config = IngestionConfig()
        self.correlation_engine = CorrelationEngine()
        
        # Processing infrastructure
        self.executor = ThreadPoolExecutor(max_workers=self.ingestion_config.max_concurrent_streams)
        self.processing_active = False
        self.metrics = {
            'total_results_processed': 0,
            'correlations_found': 0,
            'active_streams': 0,
            'average_processing_time': 0.0
        }
        
    def _setup_logging(self) -> logging.Logger:
        logger = logging.getLogger(f"IntegrationSpecialist-{self.node_id}")
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger
        
    async def connect_to_mesh(self, peer_nodes: List[str]) -> Dict[str, Any]:
        """Connect to other nodes in the mesh topology"""
        self.logger.info(f"Connecting to mesh with peers: {peer_nodes}")
        
        for peer in peer_nodes:
            self.peer_connections.add(peer)
            
        return {
            "node_id": self.node_id,
            "connected_peers": list(self.peer_connections),
            "mesh_status": "connected",
            "capabilities": [
                "real_time_result_ingestion",
                "cross_tool_correlation",
                "data_streaming",
                "result_aggregation"
            ]
        }
        
    async def setup_real_time_ingestion(self) -> Dict[str, Any]:
        """Set up real-time result ingestion pipeline"""
        self.logger.info("Setting up real-time result ingestion pipeline")
        
        # Create streams for each target tool
        target_tools = ["flake8", "pylint", "ruff", "mypy", "bandit"]
        
        for tool in target_tools:
            stream_id = f"{tool}_stream"
            self.streams[stream_id] = RealTimeStream(
                stream_id=stream_id,
                tool_name=tool,
                buffer_size=self.ingestion_config.buffer_size
            )
            
            # Subscribe to stream for correlation processing
            self.streams[stream_id].subscribe(self._process_incoming_result)
            
        # Start background processing
        self.processing_active = True
        asyncio.create_task(self._background_correlation_processor())
        
        return {
            "ingestion_status": "active",
            "streams_created": list(self.streams.keys()),
            "buffer_size": self.ingestion_config.buffer_size,
            "correlation_window": self.ingestion_config.correlation_window,
            "real_time_enabled": self.ingestion_config.enable_real_time
        }
        
    async def ingest_linter_result(self, tool_name: str, result: LinterResult) -> Dict[str, Any]:
        """Ingest a linter result into the real-time pipeline"""
        start_time = time.time()
        
        stream_id = f"{tool_name}_stream"
        if stream_id not in self.streams:
            # Create stream on-demand
            self.streams[stream_id] = RealTimeStream(
                stream_id=stream_id,
                tool_name=tool_name,
                buffer_size=self.ingestion_config.buffer_size
            )
            self.streams[stream_id].subscribe(self._process_incoming_result)
            
        # Push result to stream
        self.streams[stream_id].push_result(result)
        
        # Update metrics
        self.metrics['total_results_processed'] += 1
        processing_time = time.time() - start_time
        self._update_average_processing_time(processing_time)
        
        return {
            "ingestion_status": "success",
            "stream_id": stream_id,
            "tool_name": tool_name,
            "violations_count": len(result.violations),
            "processing_time": processing_time
        }
        
    def _process_incoming_result(self, stream_id: str, result: LinterResult) -> None:
        """Process incoming result for real-time correlation"""
        try:
            # Add to correlation engine
            self.correlation_engine.add_violations(result.tool_name, result.violations)
            
            # Trigger correlation if we have multiple tools
            if len(self.correlation_engine.violation_cache) >= 2:
                correlations = self.correlation_engine.correlate_violations()
                if correlations:
                    self.metrics['correlations_found'] += len(correlations)
                    self.logger.info(f"Found {len(correlations)} new correlations")
                    
        except Exception as e:
            self.logger.error(f"Error processing result from {stream_id}: {e}")
            
    async def _background_correlation_processor(self) -> None:
        """Background task for correlation processing"""
        while self.processing_active:
            try:
                # Periodic correlation analysis
                if len(self.correlation_engine.violation_cache) >= 2:
                    correlations = self.correlation_engine.correlate_violations()
                    
                    if correlations:
                        await self._notify_peers_of_correlations(correlations)
                        
                # Clean up old data
                await self._cleanup_old_data()
                
                # Wait for next cycle
                await asyncio.sleep(self.ingestion_config.flush_interval)
                
            except Exception as e:
                self.logger.error(f"Error in background correlation processor: {e}")
                
    async def _notify_peers_of_correlations(self, correlations: List[Dict[str, Any]]) -> None:
        """Notify peer nodes of found correlations"""
        notification = {
            "type": "correlation_update",
            "from_node": self.node_id,
            "correlations": correlations,
            "timestamp": time.time()
        }
        
        # In a real implementation, this would send to peer nodes
        self.logger.info(f"Would notify {len(self.peer_connections)} peers of {len(correlations)} correlations")
        
    async def _cleanup_old_data(self) -> None:
        """Clean up old violation data outside correlation window"""
        current_time = time.time()
        cutoff_time = current_time - self.ingestion_config.correlation_window
        
        # Clean up streams
        for stream in self.streams.values():
            if current_time - stream.last_update > self.ingestion_config.correlation_window:
                stream.active = False
                
    def _update_average_processing_time(self, processing_time: float) -> None:
        """Update rolling average processing time"""
        current_avg = self.metrics['average_processing_time']
        total_processed = self.metrics['total_results_processed']
        
        self.metrics['average_processing_time'] = (
            (current_avg * (total_processed - 1) + processing_time) / total_processed
        )
        
    async def create_result_aggregation_pipeline(self) -> Dict[str, Any]:
        """Create comprehensive result aggregation pipeline"""
        self.logger.info("Creating result aggregation pipeline")
        
        aggregation_config = {
            "aggregation_strategies": [
                "by_severity", "by_tool", "by_file", "by_category", "by_correlation"
            ],
            "output_formats": ["json", "html", "csv", "xml"],
            "real_time_updates": True,
            "batch_processing": True,
            "correlation_analysis": True
        }
        
        # Set up aggregation processors
        processors = await self._setup_aggregation_processors()
        
        return {
            "pipeline_status": "active",
            "aggregation_config": aggregation_config,
            "processors": processors,
            "streaming_enabled": self.ingestion_config.enable_real_time
        }
        
    async def _setup_aggregation_processors(self) -> Dict[str, Any]:
        """Set up result aggregation processors"""
        return {
            "severity_aggregator": {
                "description": "Aggregate violations by unified severity",
                "active": True,
                "output_format": "json"
            },
            "tool_aggregator": {
                "description": "Aggregate results by tool",
                "active": True,
                "output_format": "json"
            },
            "correlation_aggregator": {
                "description": "Aggregate correlation analysis results",
                "active": True,
                "output_format": "json"
            },
            "file_aggregator": {
                "description": "Aggregate violations by file path",
                "active": True,
                "output_format": "json"
            }
        }
        
    async def get_real_time_metrics(self) -> Dict[str, Any]:
        """Get real-time metrics and status"""
        active_streams = sum(1 for stream in self.streams.values() if stream.active)
        
        return {
            "metrics": {
                **self.metrics,
                "active_streams": active_streams,
                "total_streams": len(self.streams),
                "correlation_rules": len(self.correlation_engine.correlation_rules),
                "cached_violations": sum(len(v) for v in self.correlation_engine.violation_cache.values())
            },
            "stream_status": {
                stream_id: {
                    "active": stream.active,
                    "buffer_size": len(stream.buffer),
                    "subscribers": len(stream.subscribers),
                    "last_update": stream.last_update
                }
                for stream_id, stream in self.streams.items()
            },
            "correlation_summary": {
                "total_correlations": len(self.correlation_engine.correlation_results),
                "correlation_types": self._get_correlation_type_distribution(),
                "tool_pairs": self._get_tool_pair_correlations()
            }
        }
        
    def _get_correlation_type_distribution(self) -> Dict[str, int]:
        """Get distribution of correlation types"""
        distribution = defaultdict(int)
        for correlation in self.correlation_engine.correlation_results:
            distribution[correlation['correlation_type']] += 1
        return dict(distribution)
        
    def _get_tool_pair_correlations(self) -> Dict[str, int]:
        """Get correlations by tool pairs"""
        pairs = defaultdict(int)
        for correlation in self.correlation_engine.correlation_results:
            tool1, tool2 = correlation['tool1'], correlation['tool2']
            pair_key = f"{min(tool1, tool2)}-{max(tool1, tool2)}"
            pairs[pair_key] += 1
        return dict(pairs)
        
    async def shutdown_ingestion(self) -> Dict[str, Any]:
        """Shutdown real-time ingestion pipeline"""
        self.logger.info("Shutting down real-time ingestion pipeline")
        
        self.processing_active = False
        
        # Close all streams
        for stream in self.streams.values():
            stream.active = False
            stream.subscribers.clear()
            
        # Shutdown executor
        self.executor.shutdown(wait=True)
        
        final_metrics = await self.get_real_time_metrics()
        
        return {
            "shutdown_status": "completed",
            "final_metrics": final_metrics,
            "streams_closed": len(self.streams)
        }
        
    async def get_node_status(self) -> Dict[str, Any]:
        """Get current status of the integration specialist node"""
        return {
            "node_id": self.node_id,
            "node_type": "integration-specialist",
            "status": "active",
            "peer_connections": list(self.peer_connections),
            "processing_active": self.processing_active,
            "streams_count": len(self.streams),
            "metrics": self.metrics,
            "capabilities": [
                "real_time_result_ingestion",
                "cross_tool_correlation",
                "data_streaming",
                "result_aggregation"
            ]
        }

# Node instance for mesh coordination
integration_specialist_node = IntegrationSpecialistNode()